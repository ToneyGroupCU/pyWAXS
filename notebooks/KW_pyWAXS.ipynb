{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyWAXS\n",
    "#### Python-Based X-ray Scattering Data Reduction Notebook \n",
    "##### Toney Group, Updated: 10/24/2023, Demi's Notebook\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install pyWAXS Modules\n",
    "Restart the Kernel after you do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python setup.py clean # uncomment if pip installation fails due to false path\n",
    "%pip install -e /Users/keithwhite/github_repositories/pyWAXS # sub your pyWAXS/main directory here.\n",
    "\n",
    "''' # Tips for troubleshooting pyWAXS installation.\n",
    "# import importlib.util\n",
    "# spec = importlib.util.find_spec(\"pywaxs\") # uncomment to check the pywaxs origin information\n",
    "# print(spec)\n",
    "\n",
    "# !pip list # uncomment to check your imports list for pywaxs\n",
    "\n",
    "# import sys # uncomment to check system pathes accessed by your kernel\n",
    "# print(sys.path)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class & Module Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Custom Module Imports\n",
    "from pywaxs import WAXSTransform\n",
    "from pywaxs import WAXSSearch\n",
    "from pywaxs import WAXSReduce, WAXSTOPAS\n",
    "from pywaxs import WAXSFileManager as waxsfiles\n",
    "\n",
    "# - Additional Module Imports\n",
    "import xarray as xr # type: ignore\n",
    "import pandas as pd # type: ignore\n",
    "import numpy as np\n",
    "import pathlib, os # , copy2\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a pyWAXS Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# projectname = '1MAI_3PbI2_DMF_1M_mar23'\n",
    "projectname = '1MAI_1PbI2_DMF_0p3M_mar23'\n",
    "# projectname = '1MAI_3PbI2_DMF_1M_jun23'\n",
    "# projectname = '1MAI_1PbI2_DMF_1M_jun23'\n",
    "# projectname = 'LaB6_SRM660c_Calib'\n",
    "# projectname = 'AgBh_Calib'\n",
    "\n",
    "## --------------------------------------------------------------------------------------------------------------\n",
    "current_directory = os.getcwd() # get the current directory\n",
    "basePath = Path(current_directory) # set the basePath\n",
    "\n",
    "# Generate project paths\n",
    "projectPath, PathList = waxsfiles.generate_projectPaths(basePath, projectname)\n",
    "basePath, dataPath, poniPath, maskPath, analysisPath, poscarPath, hdf5Path, pngPath, simulationPath = PathList\n",
    "\n",
    "# Print results\n",
    "print(f\"Project Path: {projectPath}\")\n",
    "print(f\"Data Path: {dataPath}\")\n",
    "print(f\"PONI Path: {poniPath}\")\n",
    "print(f\"MASK Path: {maskPath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Metadata Keylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Calibrant\n",
    "# metadata_keylist = ['sample', # Sample Number\n",
    "#                     'saxssdd', # Sample Chemistry\n",
    "#                     'energy', # Solution Filtration\n",
    "#                     'note', # Solution Concentration\n",
    "#                     'clocktime', # Clock time @ Endstation\n",
    "#                     'xpos', # X-Position of Beam\n",
    "#                     'thpos', # Incidence Angle\n",
    "#                     'exptime', # Exposure Time\n",
    "#                     'scanID', # Scan Identification Number (BNL Specific)\n",
    "#                     'detext'] # Extension of the detector identifier at BNL 'maxs' v. 'saxs' v, 'waxs'\n",
    "\n",
    "# '''\n",
    "metadata_keylist = ['samplenum', # Sample Number\n",
    "                    'chemistry', # Sample Chemistry\n",
    "                    'filter', # Solution Filtration\n",
    "                    'concentration', # Solution Concentration\n",
    "                    'purge_rate', # Spincoater Purge Rate\n",
    "                    'substrate', # Sample Substrate\n",
    "                    'solution_volume', # Dispensed Volume\n",
    "                    'runnum', # Run Number @ Beamtime Session\n",
    "                    'clocktime', # Clock time @ Endstation\n",
    "                    'xpos', # X-Position of Beam\n",
    "                    'thpos', # Incidence Angle\n",
    "                    'exptime', # Exposure Time\n",
    "                    'scanID', # Scan Identification Number (BNL Specific)\n",
    "                    'framenum', # Frame number for tr-GIWAXS series exposures (in-situ time series)\n",
    "                    'detext'] # Extension of the detector identifier at BNL 'maxs' v. 'saxs' v, 'waxs'\n",
    "# '''\n",
    "                    \n",
    "'''\n",
    "metadata_keylist = ['samplenum', # Sample Number\n",
    "                    'chemistry', # Sample Chemistry\n",
    "                    'solvent', # Solution Filtration\n",
    "                    'concentration', # Solution Concentration\n",
    "                    'purge_rate', # Spincoater Purge Rate\n",
    "                    'substrate', # Sample Substrate\n",
    "                    'solution_volume', # Dispensed Volume\n",
    "                    'runnum', # Run Number @ Beamtime Session\n",
    "                    'clocktime', # Clock time @ Endstation\n",
    "                    'xpos', # X-Position of Beam\n",
    "                    'thpos', # Incidence Angle\n",
    "                    'exptime', # Exposure Time\n",
    "                    'scanID', # Scan Identification Number (BNL Specific)\n",
    "                    'detext'] # Extension of the detector identifier at BNL 'maxs' v. 'saxs' v, 'waxs'\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a WAXSReduce Project Instance Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - WAXSReduce Class Instantiation: Load the data, apply mask, calculate and map q-range, Ewald sphere corrections/pixel splitting, and caked pixel splitting.\n",
    "waxs_analysis = WAXSReduce(poniPath=poniPath, \n",
    "        maskPath=maskPath, \n",
    "        tiffPath=dataPath, \n",
    "        metadata_keylist=metadata_keylist,\n",
    "        energy = 12.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a .xye File for TOPAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a WAXSTOPAS instance based on the existing WAXSReduce instance\n",
    "waxs_topas = WAXSTOPAS(projectPath=projectPath, waxs_instance=waxs_analysis)\n",
    "\n",
    "# Now you can call the .xye generation method\n",
    "waxs_topas.generate_xye_file_pyfai()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the Wavelength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waxs_topas.wavelength\n",
    "\n",
    "wavelength = waxs_topas.wavelength * 1e10\n",
    "print(wavelength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a .int pXRD Simulated File for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intPath = pathlib.Path('/Users/keithwhite/github_repositories/pyWAXS/examples_local/projects/AgBh_Calib/poscar/agbh_cif.int')\n",
    "wavelength = 0.9763 # Ã…^-1\n",
    "\n",
    "def load_int_file(file_path: Path, wavelength: float) -> pd.DataFrame:\n",
    "    # Read the file, skipping the first two lines\n",
    "    df = pd.read_csv(file_path, skiprows=2, header=None, delimiter='\\s+')\n",
    "    \n",
    "    # Rename the columns\n",
    "    df.columns = ['twotheta', 'intensity', 'error']\n",
    "    \n",
    "    # Calculate q based on twotheta and wavelength\n",
    "    df['q'] = (4 * math.pi / wavelength) * np.sin(np.radians(df['twotheta'] / 2))\n",
    "    \n",
    "    # Add headers for wavelength and file path\n",
    "    df.attrs['wavelength'] = wavelength\n",
    "    df.attrs['file_path'] = str(file_path)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the data\n",
    "loaded_data = load_int_file(intPath, wavelength)\n",
    "loaded_data.head(), loaded_data.attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the Loaded (.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the figure\n",
    "%matplotlib widget\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# Boolean variable to toggle normalization\n",
    "normalize_traces = True\n",
    "\n",
    "qr_min = float(0.1)\n",
    "qr_max = float(3)\n",
    "\n",
    "filtered_data = loaded_data[(loaded_data['q'] >= qr_min) & (loaded_data['q'] <= qr_max)]\n",
    "\n",
    "if normalize_traces:\n",
    "    plt.plot(filtered_data['q'], filtered_data['intensity'] / filtered_data['intensity'].max(), label='Loaded .int File')\n",
    "else:\n",
    "    plt.plot(filtered_data['q'], filtered_data['intensity'], label='Loaded .int File')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('q / qr')\n",
    "plt.ylabel('Intensity')\n",
    "plt.title('1D Integrated Data Comparison')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overlay pXRD (.int) Calculation & Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the figure\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# Boolean variable to toggle normalization\n",
    "normalize_traces = True\n",
    "\n",
    "# methods = ['lut', 'bbox']\n",
    "methods = ['bbox']\n",
    "for method in methods:\n",
    "    waxs_analysis.run_pg_integrate1D(npt=2048, method=method, correctSolidAngle=True, polarization_factor=-0)\n",
    "    intensity_values = waxs_analysis.integrate1d_da.values\n",
    "    if normalize_traces:\n",
    "        intensity_values /= intensity_values.max()\n",
    "    plt.plot(waxs_analysis.integrate1d_da.qr, intensity_values, label=f'pygix {method}')\n",
    "\n",
    "qr_min = float(waxs_analysis.integrate1d_da.qr.min())\n",
    "qr_max = float(waxs_analysis.integrate1d_da.qr.max())\n",
    "filtered_data = loaded_data[(loaded_data['q'] >= qr_min) & (loaded_data['q'] <= qr_max)]\n",
    "\n",
    "if normalize_traces:\n",
    "    plt.plot(filtered_data['q'], filtered_data['intensity'] / filtered_data['intensity'].max(), label='Loaded .int File')\n",
    "else:\n",
    "    plt.plot(filtered_data['q'], filtered_data['intensity'], label='Loaded .int File')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('q / qr')\n",
    "plt.ylabel('Intensity')\n",
    "plt.title('1D Integrated Data Comparison')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Different Pygix 1D Integration Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare plot before loop\n",
    "plt.figure()\n",
    "plt.xlabel('qr (1/Ã…)')\n",
    "plt.ylabel('Intensity (a.u.)')\n",
    "plt.title('1D Integration: Comparison of Methods')\n",
    "\n",
    "methods = ['np', 'cython', 'bbox', 'splitpix', 'lut']\n",
    "\n",
    "plt.close('all')\n",
    "\n",
    "for method in methods:\n",
    "    # Run the 1D integration using the run_pg_integrate1D method.\n",
    "    waxs_analysis.run_pg_integrate1D(npt=2048, \n",
    "                                     method=method, \n",
    "                                     correctSolidAngle=True, \n",
    "                                     polarization_factor=None)\n",
    "\n",
    "    # Add a plot trace for this method\n",
    "    plt.plot(waxs_analysis.integrate1d_da['qr'], waxs_analysis.integrate1d_da, label=method)\n",
    "\n",
    "    # # Optionally save .xy file\n",
    "    # waxs_analysis.plot_and_save_qr_intensity(output_path=pngPath, \n",
    "    #                                          save_xy=True, \n",
    "    #                                          save_png=False)  # Disable png save here\n",
    "\n",
    "# Add legend and show the plot\n",
    "plt.legend(title='Method')\n",
    "plt.show()\n",
    "\n",
    "# Optionally save the overlay plot as a .png\n",
    "if pngPath is not None:\n",
    "    plt.savefig(pngPath / 'overlay_plot.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the Stored WAXSReduce DataArrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "title = projectname\n",
    "\n",
    "waxs_analysis.display_image(waxs_analysis.cakedtiff_xr, \n",
    "                            title=title, \n",
    "                            cmap='turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Azimuthal Integration Region on Reciprocal Space Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize class and perform azimuthal crop\n",
    "azimuth = Azimuth1D(waxs_analysis)\n",
    "# qr_range = [.85, 1]\n",
    "# qr_range = [1.68, 1.82]\n",
    "chi_range = [-90, 0]\n",
    "qr_range = [0, 3]\n",
    "img = waxs_analysis.reciptiff_xr\n",
    "cropped_data = azimuth.azimuthal_crop(azimuth.reciptiff_xr, qr_range, chi_range)\n",
    "\n",
    "# Plot the data\n",
    "azimuth.plot_data(img, cropped_data, azimuth.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1D Integration: Summing over Chi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform azimuthal integration\n",
    "num_qr_bins = 1000\n",
    "qr_bins, azimuthal_sum, gaps = azimuth.azimuthal_integration(num_qr_bins, qr_range)\n",
    "\n",
    "azimuth.plot_azimuthal_integration(qr_bins, azimuthal_sum, gaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1D Integration: Sum over qr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform azimuthal integration by chi\n",
    "num_chi_bins = 500  # Specify the number of chi bins\n",
    "chi_bins, azimuthal_sum_by_chi, gaps_by_chi = azimuth.azimuthal_integration_by_chi(num_chi_bins, chi_range)\n",
    "\n",
    "# Plot azimuthal integration by chi\n",
    "azimuth.plot_azimuthal_integration_by_chi(chi_bins, azimuthal_sum_by_chi, gaps_by_chi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Analysis Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sin(chi) Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caked Image Sin-Chi Correction: Apply sin(chi) correction to the caked image.\n",
    "cakedtiff_sinchi_xr = waxs_analysis.sinchi_corr(chicorr = True, \n",
    "                                                qsqr = False)\n",
    "\n",
    "# Image Normalization: Normalize the caked image.\n",
    "cakedtiff_xr_norm, (max_x, max_y) = waxs_analysis.normalize_image(img = waxs_analysis.cakedtiff_sinchi_xr, \n",
    "                                                  normalizerecip=False)\n",
    "data = cakedtiff_xr_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Folding: Fold the caked image.\n",
    "folded_data = waxs_analysis.fold_image(data, 'chi')\n",
    "data = folded_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = waxs_analysis.cakedtiff_xr\n",
    "\n",
    "# Image Normalization: Normalize the caked image.\n",
    "data, (max_x, max_y) = waxs_analysis.normalize_image(img = data, \n",
    "                                                  normalizerecip=False)\n",
    "\n",
    "# Image Interpolation: Interpolate the folded image.\n",
    "# interpolator = ImageInterpolator() # Create the interpolator object. \n",
    "# data = interpolator.simple_interpolate(data, 'horizontal', 'slinear') # interpolate horizontal gaps with slinear method.\n",
    "# data = interpolator.simple_interpolate(data, 'vertical', 'slinear') # interpolate unfilled vertical gaps with slinear method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Folding: Fold the caked image.\n",
    "data = waxs_analysis.cakedtiff_xr\n",
    "data = waxs_analysis.fold_image(data, 'chi')\n",
    "data, (max_x, max_y) = waxs_analysis.normalize_image(img = data, \n",
    "                                                  normalizerecip=False)\n",
    "\n",
    "# Display Image\n",
    "waxs_analysis.display_image(data, \n",
    "                            title='folded', \n",
    "                            cmap='turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Peak Finding\n",
    "Find the peaks using a our custom peak finding algorithm.\n",
    "\n",
    "Parameters:\n",
    "- sigma1 (float, default=1.0): The standard deviation for the first Gaussian filter.\n",
    "- sigma2 (float, default=2.0): The standard deviation for the second Gaussian filter.\n",
    "- threshold (float, default=0.2): Threshold for initial peak identification.\n",
    "- clustering_method (str, default='DBSCAN'): The clustering method to use ('DBSCAN' or 'HDBSCAN').\n",
    "- eps (float, default=3): The maximum distance between two samples for them to be considered as in the same cluster (DBSCAN).\n",
    "- min_samples (int, default=2): The number of samples in a neighborhood for a point to be considered as a core point (DBSCAN).\n",
    "- k (int, default=3): The number of nearest neighbors to consider for the recentering algorithm.\n",
    "- radius (float, default=5): The radius within which to search for neighbors in the recentering algorithm.\n",
    "- edge_percentage (float, default=5): The percentage of the minimum edge length to be considered as the edge zone.\n",
    "- stricter_threshold (float, default=0.01): A stricter threshold for edge peaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "%matplotlib widget\n",
    "\n",
    "# Find peaks (implement the actual peak-finding logic in the find_peaks method)\n",
    "peak_finder = WAXSSearch(data) # create the WAXSSearch object\n",
    "\n",
    "# NOTE: Make sure you pass the ACTIVE DataArray you are working on, notice how I passed 'data' \n",
    "# since this is the processed dataset we are working with locally.\n",
    "\n",
    "dataset = peak_finder.waxssearch_main(sigma1=.1,\n",
    "                                  sigma2=1.5,\n",
    "                                  threshold=.015,\n",
    "                                  clustering_method='HDBSCAN',\n",
    "                                  eps=1, \n",
    "                                  min_samples=2,\n",
    "                                  k=8,\n",
    "                                  radius=10,\n",
    "                                  edge_percentage=2,\n",
    "                                  stricter_threshold=20)\n",
    "\n",
    "# Display image with peaks\n",
    "peak_finder.display_image_with_peaks_and_DoG(dataset,\n",
    "                                       title='peak finder', \n",
    "                                       cmap='turbo')\n",
    "\n",
    "# peak_finder.save_to_netcdf(hdf5Path) # Creates an hdf5 file with the name 'output.nc' in your output path. Will generalize\n",
    "# the file naming convention for this soon.\n",
    "'''\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "data = waxs_analysis.reciptiff_xr\n",
    "# data = waxs_analysis.fold_image(data, 'chi')\n",
    "# data, (max_x, max_y) = waxs_analysis.normalize_image(img = data, \n",
    "#                                                   normalizerecip=False)\n",
    "\n",
    "# Find peaks (implement the actual peak-finding logic in the find_peaks method)\n",
    "peak_finder = WAXSSearch(data) # create the WAXSSearch object\n",
    "\n",
    "# NOTE: Make sure you pass the ACTIVE DataArray you are working on, notice how I passed 'data' \n",
    "# since this is the processed dataset we are working with locally.\n",
    "\n",
    "const_params = {\n",
    "    'threshold': 0.0005,\n",
    "    'clustering_method': 'HDBSCAN',\n",
    "    'eps': 1,\n",
    "    'min_samples': 2,\n",
    "    'k': 4,\n",
    "    'radius': 2,\n",
    "    'edge_percentage': 2,\n",
    "    'stricter_threshold': 20\n",
    "}\n",
    "\n",
    "dataset = peak_finder.waxssearch_main(sigma1=.15,\n",
    "                                        sigma2=2.91,\n",
    "                                        **const_params)\n",
    "\n",
    "# Display image with peaks\n",
    "peak_finder.display_image_with_peaks_and_DoG(dataset,\n",
    "                                       title='peak finder', \n",
    "                                       cmap='turbo')\n",
    "\n",
    "# peak_finder.save_to_netcdf(hdf5Path) # Creates an hdf5 file with the name 'output.nc' in your output path. Will generalize\n",
    "# the file naming convention for this soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Peak Search Parameter Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Peak Search Parameter Sensitivity Analysis\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Import the WAXSSearch class and any other dependencies here\n",
    "\n",
    "# Define constant parameters\n",
    "const_params = {\n",
    "    'threshold': 0.0005,\n",
    "    'clustering_method': 'HDBSCAN',\n",
    "    'eps': 1,\n",
    "    'min_samples': 2,\n",
    "    'k': 4,\n",
    "    'radius': 2,\n",
    "    'edge_percentage': 2,\n",
    "    'stricter_threshold': 5\n",
    "}\n",
    "\n",
    "# Assume that 'data' is available here or load it\n",
    "\n",
    "# Define ranges for sigma1 and sigma2\n",
    "sigma1_values = np.linspace(0.1, 1.2, 20)\n",
    "sigma2_values = np.linspace(1.3, 3, 20)\n",
    "\n",
    "# Create a matrix to store the results\n",
    "num_peaks_matrix = np.zeros((len(sigma1_values), len(sigma2_values)))\n",
    "\n",
    "# Run the simulation\n",
    "for i, sigma1 in enumerate(sigma1_values):\n",
    "    for j, sigma2 in enumerate(sigma2_values):\n",
    "        # Create a WAXSSearch object with the current 'data'\n",
    "        peak_finder = WAXSSearch(data)\n",
    "        \n",
    "        # Run the actual waxssearch_main method\n",
    "        dataset = peak_finder.waxssearch_main(sigma1=sigma1,\n",
    "                                              sigma2=sigma2,\n",
    "                                              **const_params)\n",
    "        \n",
    "        # Extract the number of peaks found\n",
    "        num_peaks = np.count_nonzero(dataset['peak_positions'].values == 1)\n",
    "        num_peaks_matrix[i, j] = num_peaks\n",
    "\n",
    "\n",
    "# Convert to DataFrame for better annotation in heatmap\n",
    "df_num_peaks = pd.DataFrame(num_peaks_matrix, index=np.round(sigma1_values, 2), columns=np.round(sigma2_values, 2))\n",
    "\n",
    "plt.close('all')\n",
    "\n",
    "# Generate the heatmap\n",
    "sns.heatmap(df_num_peaks, annot=True, fmt=\".0f\", cmap=\"YlGnBu\")\n",
    "plt.xlabel(\"Sigma2 Values\")\n",
    "plt.ylabel(\"Sigma1 Values\")\n",
    "plt.title(\"Heatmap of Number of Peaks Detected\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio  # for creating GIF\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Import WAXSSearch and other dependencies here\n",
    "# ...\n",
    "\n",
    "# Assume that 'data' is available here or load it\n",
    "# ...\n",
    "\n",
    "# Define constant parameters\n",
    "const_params = {\n",
    "    'clustering_method': 'HDBSCAN',\n",
    "    'eps': 1,\n",
    "    'min_samples': 2,\n",
    "    'k': 4,\n",
    "    'radius': 2,\n",
    "    'edge_percentage': 2,\n",
    "    'stricter_threshold': 5\n",
    "}\n",
    "\n",
    "# Define ranges for sigma1, sigma2, and threshold\n",
    "sigma1_values = np.linspace(0.1, 1.2, 20)\n",
    "sigma2_values = np.linspace(1.3, 3, 20)\n",
    "threshold_values = np.linspace(0.0001, 0.001, 10)  # Replace with your range\n",
    "\n",
    "# Initialize a list to store heatmap frames\n",
    "frames = []\n",
    "\n",
    "# Loop over different threshold values\n",
    "for t, threshold in enumerate(threshold_values):\n",
    "    num_peaks_matrix = np.zeros((len(sigma1_values), len(sigma2_values)))\n",
    "\n",
    "    for i, sigma1 in enumerate(sigma1_values):\n",
    "        for j, sigma2 in enumerate(sigma2_values):\n",
    "            if sigma1 >= sigma2:\n",
    "                continue\n",
    "\n",
    "            peak_finder = WAXSSearch(data)\n",
    "            dataset = peak_finder.waxssearch_main(sigma1=sigma1, sigma2=sigma2, threshold=threshold, **const_params)\n",
    "            num_peaks = np.count_nonzero(dataset['peak_positions'].values == 1)\n",
    "            num_peaks_matrix[i, j] = num_peaks\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df_num_peaks = pd.DataFrame(num_peaks_matrix, index=np.round(sigma1_values, 2), columns=np.round(sigma2_values, 2))\n",
    "    \n",
    "    # Generate and save the heatmap\n",
    "    plt.figure()\n",
    "    sns.heatmap(df_num_peaks, annot=True, fmt=\".0f\", cmap=\"YlGnBu\")\n",
    "    plt.xlabel(\"Sigma2 Values\")\n",
    "    plt.ylabel(\"Sigma1 Values\")\n",
    "    plt.title(f\"Heatmap of Number of Peaks Detected (Threshold={threshold})\")\n",
    "    \n",
    "    # Save as PNG\n",
    "    plt.savefig(f\"heatmap_{t}.png\")\n",
    "    \n",
    "    # Append to frames for GIF\n",
    "    frames.append(imageio.imread(f\"heatmap_{t}.png\"))\n",
    "\n",
    "# Create GIF\n",
    "imageio.mimsave('heatmap.gif', frames, duration=1)  # 1-second duration for each frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "# Display GIF in Jupyter Notebook\n",
    "with open('heatmap.gif','rb') as file:\n",
    "    display(Image(file.read()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import dask\n",
    "from dask import delayed, compute\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def run_peak_finder(sigma1, sigma2, data, threshold, const_params):\n",
    "    if sigma1 >= sigma2:\n",
    "        return None\n",
    "    peak_finder = WAXSSearch(data)\n",
    "    dataset = peak_finder.waxssearch_main(sigma1=sigma1, sigma2=sigma2, threshold=threshold, **const_params)\n",
    "    num_peaks = np.count_nonzero(dataset['peak_positions'].values == 1)\n",
    "    return num_peaks\n",
    "\n",
    "def generate_heatmap(matrix, sigma1_values, sigma2_values, threshold):\n",
    "    df_num_peaks = pd.DataFrame(matrix, index=np.round(sigma1_values, 2), columns=np.round(sigma2_values, 2))\n",
    "    plt.figure()\n",
    "    sns.heatmap(df_num_peaks, annot=True, fmt=\".0f\", cmap=\"YlGnBu\")\n",
    "    plt.xlabel(\"Sigma2 Values\")\n",
    "    plt.ylabel(\"Sigma1 Values\")\n",
    "    plt.title(f\"Heatmap of Number of Peaks Detected (Threshold={threshold})\")\n",
    "    plt.savefig(f\"heatmap_{threshold}.png\")\n",
    "    plt.close()\n",
    "\n",
    "def create_gif(frames, filename='heatmap.gif', duration=1):\n",
    "    imageio.mimsave(filename, frames, duration=duration)\n",
    "\n",
    "# Main logic\n",
    "# Define constant parameters and ranges\n",
    "# Define constant parameters\n",
    "const_params = {\n",
    "    'clustering_method': 'HDBSCAN',\n",
    "    'eps': 1,\n",
    "    'min_samples': 2,\n",
    "    'k': 4,\n",
    "    'radius': 2,\n",
    "    'edge_percentage': 2,\n",
    "    'stricter_threshold': 5\n",
    "}\n",
    "\n",
    "sigma1_values = np.linspace(0.1, 1.2, 20)\n",
    "sigma2_values = np.linspace(1.3, 3, 20)\n",
    "# threshold_values = np.linspace(0.0001, 0.001, 10)\n",
    "threshold_values = np.linspace(0.0006, 0.001, 3)\n",
    "data = waxs_analysis.reciptiff_xr\n",
    "frames = []\n",
    "\n",
    "for t, threshold in enumerate(threshold_values):\n",
    "    tasks = []\n",
    "    for sigma1 in sigma1_values:\n",
    "        for sigma2 in sigma2_values:\n",
    "            task = delayed(run_peak_finder)(sigma1, sigma2, data, threshold, const_params)\n",
    "            tasks.append(task)\n",
    "\n",
    "    num_peaks_values = compute(*tasks)\n",
    "    num_peaks_matrix = np.array(num_peaks_values).reshape(len(sigma1_values), len(sigma2_values))\n",
    "    num_peaks_matrix = np.where(num_peaks_matrix == None, 0, num_peaks_matrix)\n",
    "\n",
    "    generate_heatmap(num_peaks_matrix, sigma1_values, sigma2_values, threshold)\n",
    "    frame = imageio.imread(f\"heatmap_{threshold}.png\")\n",
    "    frames.append(frame)\n",
    "\n",
    "create_gif(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Existing Project File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# waxsPath = pathlib.Path('/Users/keithwhite/github_repositories/pyWAXS/examples_local/projects/1MAI_3PbI2_DMF_1M_mar23/analysis/hdf5/3MAI_1PbI2_DMF_1M_mar23_peaks.nc')\n",
    "waxsPath = pathlib.Path('/Users/keithwhite/github_repositories/pyWAXS/examples_local/projects/1MAI_3PbI2_DMF_1M_mar23/analysis/hdf5/output.nc')\n",
    "\n",
    "waxs_ds = waxs_analysis.load_xarray_dataset(waxsPath)\n",
    "\n",
    "waxs_ds.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waxs_ds_peaks = WAXSSearch(waxs_ds) # create the WAXSSearch object\n",
    "\n",
    "# Display image with peaks\n",
    "waxs_ds_peaks.display_image_with_peaks(waxs_analysis.ds,\n",
    "                                       title='manual peaks', \n",
    "                                       cmap='turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D Integration Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrator = Integration1D(waxs_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your integrator object 'integrator' has access to and references your active 'waxs_analysis' session. \n",
    "\n",
    "It also has access to all of the WAXSReduce() methods. You can prove it to yourself like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Cake Slide 1D: Display 2D Caked Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrator.display_image(integrator.cakedtiff_xr, # Display the caked data we will manipulate with the 1D image processing.\n",
    "                            title='processed', \n",
    "                            cmap='turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Cake Slice 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrator = Integration1D(waxs_analysis)\n",
    "\n",
    "integrator.cakeslice1D(integrator.cakedtiff_xr, chislice=[-90, 90], qrslice=[0, 4], cakeslicesum='chi')\n",
    "\n",
    "integrator.display_image1D(integrator.cakeslice1D_xr, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Boxcut 1D: Display 2D Recip Space Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "# integrator = Integration1D(waxs_analysis)\n",
    "integrator.display_image(integrator.reciptiff_xr, # Display the caked data we will manipulate with the 1D image processing.\n",
    "                            title='recip space map', \n",
    "                            cmap='turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Box Cut 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated `process_slices` function to dynamically generate a sample name prefix based on the slices\n",
    "def process_slices(integrator, base_samplenameprefix, pngPath, plot_interpolated=False, interpolate_gaps = False, interp_method = 'linear', order = None):\n",
    "    # Qz Layer Lines\n",
    "    qxyslice_qz = [-2.5, 2.5]\n",
    "    qzslice_qz_list = [\n",
    "        [.42, .57],\n",
    "        [.9, 1.07],\n",
    "        [1.42, 1.58],\n",
    "        [1.83, 2.16],\n",
    "        [2.4, 2.6]\n",
    "    ]\n",
    "    \n",
    "    for qzslice in qzslice_qz_list:\n",
    "        dynamic_samplenameprefix = f\"{base_samplenameprefix}_qxyrange{qxyslice_qz[0]}_{qxyslice_qz[1]}_qzrange{qzslice[0]}_{qzslice[1]}\"\n",
    "        title = generate_title('qz', qxyslice_qz, qzslice)\n",
    "        integrator.boxcut1D(integrator.reciptiff_xr, qxyslice=qxyslice_qz, qzslice=qzslice, boxcutsum='qz', interpolate_gaps=interpolate_gaps, interp_method = interp_method, order = None)\n",
    "        integrator.display_image1D(integrator, color='blue', title=title, save_image=True, samplenameprefix=dynamic_samplenameprefix, savePath=pngPath, plot_interpolated=plot_interpolated)\n",
    "        \n",
    "    # Qxy Layer Lines\n",
    "    qzslice_qxy = [0, 3]\n",
    "    qxyslice_qxy_list = [\n",
    "        [-0.95, -0.7],\n",
    "        [-1.18, -1.08],\n",
    "        [-1.7, -1.45],\n",
    "        [-2.06, -1.9],\n",
    "        [-2.24, -2.1],\n",
    "        [-2.36, -2.26]\n",
    "    ]\n",
    "    \n",
    "    for qxyslice in qxyslice_qxy_list:\n",
    "        dynamic_samplenameprefix = f\"{base_samplenameprefix}_qxyrange{qxyslice[0]}_{qxyslice[1]}_qzrange{qzslice_qxy[0]}_{qzslice_qxy[1]}\"\n",
    "        title = generate_title('qxy', qxyslice, qzslice_qxy)\n",
    "        integrator.boxcut1D(integrator.reciptiff_xr, qxyslice=qxyslice, qzslice=qzslice_qxy, boxcutsum='qxy', interpolate_gaps=interpolate_gaps, interp_method = interp_method)\n",
    "        integrator.display_image1D(integrator, color='red', title=title, save_image=True, samplenameprefix=dynamic_samplenameprefix, savePath=pngPath, plot_interpolated=plot_interpolated)\n",
    "        \n",
    "# The generate_title function for reference\n",
    "def generate_title(boxcutsum, qxyslice=None, qzslice=None):\n",
    "    if boxcutsum == 'qz':\n",
    "        title = f\"Qz Layer Lines (Qxy: {qzslice[0]} to {qzslice[1]})\"\n",
    "    elif boxcutsum == 'qxy':\n",
    "        title = f\"Qxy Layer Lines (Qz: {qxyslice[0]} to {qxyslice[1]})\"\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_samplenameprefix = '3MAI1PbI2_DMF_1M'\n",
    "# interp_method = 'cubic', 'slinear', 'nearest', 'zero', 'linear'\n",
    "interp_method = 'linear'\n",
    "process_slices(integrator, base_samplenameprefix, pngPath, plot_interpolated=False, interpolate_gaps = False, interp_method = 'linear', order = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Pole Figure 1D: Display 2D Caked Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrator.display_image(integrator.cakedtiff_xr, # Display the caked data we will manipulate with the 1D image processing.\n",
    "                            title='processed', \n",
    "                            cmap='turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Pole Figure 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrator.polefig1D(integrator.cakedtiff_xr, pole_chislice=[-90, 90], pole_qrslice=[1.8, 1.92], qrcenter=1.85, chicenter=0, poleleveler='linear')\n",
    "integrator.polefig1D(integrator.cakedtiff_xr, pole_chislice=[-90, 90], pole_qrslice=[.62, .75], qrcenter=.67, chicenter=0, poleleveler='average')\n",
    "# integrator.polefig1D(integrator.cakedtiff_xr, pole_chislice=[-90, 90], pole_qrslice=[.62, .75], qrcenter=.67, chicenter=0, poleleveler=None)\n",
    "# integrator.polefig1D(integrator.cakedtiff_xr, pole_chislice=[-90, 90], pole_qrslice=[.62, .75], qrcenter=.67, chicenter=0, poleleveler='linear')\n",
    "\n",
    "integrator.display_image1D(integrator.polefig1D_xr, color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) Azimuthal Integration: Display the 2D Recip Space Map\n",
    "This is effectively the same as the caked integration, but with our own custom pixel splitting routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrator.display_image(integrator.reciptiff_xr, # Display the caked data we will manipulate with the 1D image processing.\n",
    "                            title='recip space map', \n",
    "                            cmap='turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Setup DataPaths & Metadata Keys Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Base Path Definitions -- #\n",
    "basePath = pathlib.Path('/Users/keithwhite/github_repositories/pyWAXS/example')\n",
    "dataPath = basePath.joinpath('data') # Generalized path for data.\n",
    "poniPath = basePath.joinpath('poni') # PONI (.poni) File\n",
    "maskPath = basePath.joinpath('mask') # MASK (.edf, .json)\n",
    "outputPath = basePath.joinpath('output') # Generalized for file outputs\n",
    "\n",
    "# -- Example Specific Paths -- #\n",
    "# TIFF File w/ Data\n",
    "tiffPath = dataPath.joinpath('sam22_1MAI1PbI2_unfilt_0p3M_5p0scfh_Si_30uL_043_2068.2s_x0.015_th0.300_0.49s_986546_001639_maxs.tiff')\n",
    "# PONI File Calibrant\n",
    "poniPath = poniPath.joinpath('may23_poni4_nslsiimar23_12p7keV_CeO_KWPos_mask5_fit2.poni')\n",
    "# Image Mask File\n",
    "maskPath = maskPath.joinpath('may23_nslsiimar23_12p7keV_AgBh_KWPos_wSi_th0p3_mask_5.edf')\n",
    "# Output for HDF5 File Format (a nicely formatted output file we will use)\n",
    "hdf5Path = outputPath.joinpath('hdf5') # output path for generated hdf5 files\n",
    "\n",
    "# -- TIFF Path & Corresponding Metadata Keylist -- #\n",
    "# Look at your filename, each '_' delimiter spaces out two keys. Add one key for each position in the filename.\n",
    "metadata_keylist = ['samplenum', # Sample Number\n",
    "                    'chemistry', # Sample Chemistry\n",
    "                    'filter', # Solution Filtration\n",
    "                    'concentration', # Solution Concentration\n",
    "                    'purge_rate', # Spincoater Purge Rate\n",
    "                    'substrate', # Sample Substrate\n",
    "                    'solution_volume', # Dispensed Volume\n",
    "                    'runnum', # Run Number @ Beamtime Session\n",
    "                    'clocktime', # Clock time @ Endstation\n",
    "                    'xpos', # X-Position of Beam\n",
    "                    'thpos', # Incidence Angle\n",
    "                    'exptime', # Exposure Time\n",
    "                    'scanID', # Scan Identification Number (BNL Specific)\n",
    "                    'framenum', # Frame number for tr-GIWAXS series exposures (in-situ time series)\n",
    "                    'detext'] # Extension of the detector identifier at BNL 'maxs' v. 'saxs' v, 'waxs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -- Core Path Definitions -- #\n",
    "# basePath = pathlib.Path('/Users/keithwhite/github_repositories/pyWAXS/examples')\n",
    "# calibPath = basePath.joinpath('calib_files/ponimask')\n",
    "# dataPath = basePath.joinpath('data_files')\n",
    "\n",
    "# # -- Project Specific Paths -- #\n",
    "# # - March 2023 @ 11-BM (CMS)\n",
    "# poniPath = calibPath.joinpath('may23/may23_poni4_nslsiimar23_12p7keV_CeO_KWPos_mask5_fit2.poni')\n",
    "# maskPath = calibPath.joinpath('may23/may23_nslsiimar23_12p7keV_AgBh_KWPos_wSi_th0p3_mask_5.edf')\n",
    "# zarrPath = basePath.joinpath('output_files/zarr_files')\n",
    "\n",
    "# # -- TIFF Path & Corresponding Metadata Keylist -- #\n",
    "# # Look at your filename, each '_' delimiter spaces out two keys. Add one key for each position in the filename.\n",
    "# metadata_keylist = ['samplenum', \n",
    "#                     'chemistry', \n",
    "#                     'filter', \n",
    "#                     'concentration', \n",
    "#                     'purge_rate', \n",
    "#                     'substrate', \n",
    "#                     'solution_volume', \n",
    "#                     'runnum', \n",
    "#                     'clocktime', \n",
    "#                     'xpos', \n",
    "#                     'thpos', \n",
    "#                     'exptime', \n",
    "#                     'scanID', \n",
    "#                     'framenum', \n",
    "#                     'detext']\n",
    "\n",
    "# tiffPath = dataPath.joinpath('sam22_1MAI1PbI2_unfilt_0p3M_5p0scfh_Si_30uL_043_2068.2s_x0.015_th0.300_0.49s_986546_001639_maxs.tiff')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Create Project Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - WAXSReduce Class Instantiation\n",
    "waxs_analysis = WAXSReduce(poniPath=poniPath, \n",
    "        maskPath=maskPath, \n",
    "        tiffPath=tiffPath, \n",
    "        metadata_keylist=metadata_keylist,\n",
    "        energy = 12.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notice how we loaded all of the following: image correction file references, the X-ray energy, and the data?\n",
    "\n",
    "Assuming your file has the incidence angle in the sample name, we can generate accurately corrected images. You can now access the reciprocal space image corrections and caked image corrections:\n",
    "\n",
    "Reciprocal Space Map:\n",
    "```python\n",
    "waxs_analysis.reciptiff_xr\n",
    "```\n",
    "\n",
    "Caked Image:\n",
    "```python\n",
    "waxs_analysis.cakedtiff_xr\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waxs_analysis.reciptiff_xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note how we had to use the syntax:\n",
    "```python\n",
    "waxs_analysis.\n",
    "```\n",
    "##### to access the the data.\n",
    "This is how we access the instance that refers to your analysis project. You can have multiple instances coexist in the same notebook.\n",
    "\n",
    "Now, click the 'Attributes' dropdown above. We loaded the data into an XArray DataArray (DA). \n",
    "##### All of the keys listed above in the setup were used to extract your metadata from the filename."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Image Plotting Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Display Image (Generalized Plotting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "# -- Display the Caked Image\n",
    "# waxs_analysis.cakeddisplay_xr() # method 1\n",
    "\n",
    "# waxs_analysis.display_image(waxs_analysis.cakedtiff_xr, # method 2\n",
    "waxs_analysis.display_image(waxs_analysis.cakedtiff_xr, \n",
    "                            title='caked image', \n",
    "                            cmap='turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### You can plot the data stored as attributes with one of a set of built-in methods, or a generalized method.\n",
    "\n",
    "Custom Method (Caked Image):\n",
    "```python\n",
    "waxs_analysis.cakeddisplay_xr()\n",
    "```\n",
    "General Method Equivalent:\n",
    "```python\n",
    "waxs_analysis.display_image(waxs_analysis.cakedtiff_xr, \n",
    "                            title='Caked Image', \n",
    "                            cmap='turbo')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Quick Plotting Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (i) Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "waxs_analysis.rawdisplay_xr()\n",
    "# waxs_analysis.rawtiff_xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (ii) Reciprocal Space Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "waxs_analysis.recipdisplay_xr()\n",
    "# waxs_analysis.reciptiff_xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (iii) Caked Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "waxs_analysis.cakeddisplay_xr()\n",
    "# waxs_analysis.cakedtiff_xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 2D Image Processing Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Image Intensity Correction (sin(chi)) Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "cakedtiff_sinchi_xr = waxs_analysis.sinchi_corr(chicorr = True, \n",
    "                                                qsqr = False)\n",
    "\n",
    "waxs_analysis.display_image(waxs_analysis.cakedtiff_sinchi_xr, \n",
    "                            title='caked image, sin(chi) corrected', \n",
    "                            cmap='turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Image Normalization Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "# Generate the normalized reciprocal space map image\n",
    "cakedtiff_xr_norm, (max_x, max_y) = waxs_analysis.normalize_image(img = waxs_analysis.cakedtiff_sinchi_xr, \n",
    "                                                  normalizerecip=False)\n",
    "\n",
    "# Check if the returned image is None or not of a compatible type\n",
    "if cakedtiff_xr_norm is None or not isinstance(cakedtiff_xr_norm, (np.ndarray, xr.DataArray)):\n",
    "    raise ValueError(\"The normalized image is None or not of a compatible type.\")\n",
    "\n",
    "print ('Maximum Intensity Pixel Coordinate: ', 'x: ', max_x, 'y: ', max_y)\n",
    "\n",
    "# Display the normalized image\n",
    "waxs_analysis.display_image(cakedtiff_xr_norm, \n",
    "                            title='caked, sin(chi) corrected, normalized', \n",
    "                            cmap='turbo')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Image Folding Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a WAXS_Analyze instance and test the fold_image method\n",
    "# waxs = WAXS_Analyze()\n",
    "# original_data = .cakedtiff_xr.copy()\n",
    "# folded_data = waxs.fold_image(waxs.cakedtiff_xr, 'chi')\n",
    "\n",
    "original_data = cakedtiff_xr_norm\n",
    "# original_data = waxs_analysis.cakedtiff_xr\n",
    "folded_data = waxs_analysis.fold_image(original_data, 'chi')\n",
    "\n",
    "# Display the original and folded data for validation\n",
    "# original_data, folded_data\n",
    "\n",
    "waxs_analysis.display_image(folded_data, \n",
    "                            title='caked, sin(chi) corrected, normalized, folded', \n",
    "                            cmap='turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Image Interpolation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "data = folded_data\n",
    "\n",
    "interpolator = ImageInterpolator()\n",
    "data = interpolator.simple_interpolate(data, 'horizontal', 'slinear')\n",
    "data = interpolator.simple_interpolate(data, 'vertical', 'slinear')\n",
    "# data = interpolator.patch_interpolate(data)\n",
    "\n",
    "# interpolator.save_dataarray_to_netcdf(interpolated_img, 'test_file')\n",
    "\n",
    "waxs_analysis.display_image(data, \n",
    "                            title='interpolated', \n",
    "                            cmap='turbo')\n",
    "\n",
    "''' # -- Testing Different Interpolation Algorithms\n",
    "# interpolated_img = interpolator.patch_interpolate(data)\n",
    "# interp_horizontal = interpolator.linear_interpolate(data, 'horizontal')\n",
    "# interp_vertical = interpolator.linear_interpolate(data, 'vertical')\n",
    "# interpolated = interpolator.simple_interpolate(data, 'vertical', 'linear')\n",
    "\n",
    "# # Create an instance of the ImageInterpolator class\n",
    "# interpolator = ImageInterpolator()\n",
    "\n",
    "# # Define methods and directions\n",
    "# # methods = [\"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\",\n",
    "# #            \"polynomial\", \"barycentric\", \"krog\", \"pchip\", \"spline\", \"akima\"]\n",
    "# methods = [\"linear\", \"nearest\", \"zero\", \"slinear\", \"akima\"]\n",
    "# directions = [\"vertical\", \"horizontal\"]\n",
    "\n",
    "# # Let's visualize all the interpolated results to understand how each method performs.\n",
    "\n",
    "# # Dictionary to store the results\n",
    "# interpolated_results = {}\n",
    "\n",
    "# # Iterate through all combinations of methods and directions to perform interpolation\n",
    "# for method in methods:\n",
    "#     for direction in directions:\n",
    "#         try:\n",
    "#             interpolated_img = interpolator.simple_interpolate(data.copy(), direction, method)\n",
    "#             key = f\"{method}_{direction}\"\n",
    "#             interpolated_results[key] = interpolated_img\n",
    "#         except Exception as e:\n",
    "#             print(f\"Skipped {method} in {direction} direction due to error: {e}\")\n",
    "\n",
    "# # Let's visualize all the interpolated results to understand how each method performs.\n",
    "# fig, axes = plt.subplots(len(methods), len(directions), figsize=(20, 40))\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# for i, (key, data) in enumerate(interpolated_results.items()):\n",
    "#     ax = axes[i]\n",
    "#     data.plot(ax=ax, add_colorbar=False, cmap='turbo')\n",
    "#     ax.set_title(f\"Method: {key}\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e) Image Smoothing Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.colors import LogNorm\n",
    "# cmap = plt.cm.turbo\n",
    "# cmap.set_bad('black')\n",
    "\n",
    "waxs_analysis.smooth_image(folded_data, method='gaussian', sigma=1e-6)\n",
    "\n",
    "smoothed_image, (max_x, max_y) = waxs_analysis.normalize_image(img = waxs_analysis.smoothed_img, normalizerecip=False)\n",
    "\n",
    "print ('Maximum Intensity Pixel Coordinate: ', 'x: ', max_x, 'y: ', max_y)\n",
    "\n",
    "# Display the smoothed image with appropriate coordinates using the 'display_image' method\n",
    "# plt = waxs_analysis.display_image(waxs_analysis.smoothed_img, \n",
    "waxs_analysis.display_image(waxs_analysis.smoothed_img, \n",
    "                            title='caked, sin(chi) corrected, normalized, folded, smoothed', \n",
    "                            cmap='turbo')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (f) 2D Image Peak Finding Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "data = None\n",
    "data = folded_data\n",
    "# Find peaks (implement the actual peak-finding logic in the find_peaks method)\n",
    "\n",
    "data = waxs_analysis.find_peaks_DoG (data, \n",
    "                                     sigma1=.4, \n",
    "                                     sigma2=2, \n",
    "                                     threshold=0.008, \n",
    "                                     clustering_method='HDBSCAN', \n",
    "                                     eps=1, \n",
    "                                     min_samples=2, \n",
    "                                     k=3, \n",
    "                                     radius=5)\n",
    "\n",
    "# Display image with peaks\n",
    "waxs_analysis.display_image_with_peaks_and_DoG(data,\n",
    "                                       title='peak finder', \n",
    "                                       cmap='turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) 1D Data Reduction (Integration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "cmap = plt.cm.turbo\n",
    "cmap.set_bad('black')\n",
    "plt.close('all')\n",
    "\n",
    "# DA = waxs_analysis.cakedtiff_xr\n",
    "# DA = cakedtiff_sinchi_xr\n",
    "DA = waxs_analysis.smoothed_img\n",
    "# DA, (max_x, max_y) = waxs_analysis.normalize_image(img = waxs_analysis.cakedtiff_xr, normalizerecip=False)\n",
    "\n",
    "colors = cmap(np.linspace(0,1,10))\n",
    "# fig, axs = plt.subplots(1, 2, figsize=(9,3))\n",
    "# plot.line(ax=axs[0], color=colors[i])\n",
    "\n",
    "axs = DA.sum('chi').sel(method='nearest').sel(qr=slice(0.2,3)).plot.line(color=colors[9])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Saving/Exporting & Loading Project Sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Exporting a Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basePath = pathlib.Path('/Users/keithwhite/github_repositories/pyWAXS/examples')\n",
    "zarrPath = basePath.joinpath('output_files/zarr_files')\n",
    "projectName = 'test_project'\n",
    "waxs_analysis.exportzarr(zarrPath=zarrPath, projectName=projectName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Load an Existing Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basePath = pathlib.Path('/Users/keithwhite/github_repositories/pyWAXS/examples')\n",
    "zarrPath = basePath.joinpath('output_files/zarr_files')\n",
    "projectName = 'test_project'\n",
    "waxs_analysis = WAXSReduce(zarrPath = zarrPath, projectName = projectName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Smoothing Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "# Smooth the image using the 'smooth_image' method\n",
    "# smoothed_image = waxs_analysis.smooth_image(waxs_analysis.reciptiff_xr.values, \n",
    "#                                             method='gaussian', \n",
    "#                                             sigma=.05)\n",
    "\n",
    "smoothed_image = waxs_analysis.smooth_image(waxs_analysis.reciptiff_xr, \n",
    "                                            method='gaussian', \n",
    "                                            sigma=1)\n",
    "# smoothed_image = waxs_analysis.normalize_image()\n",
    "# smoothed_image = waxs_analysis.smooth_image(smoothed_image, method='total_variation', sigma=100)\n",
    "\n",
    "# Display the smoothed image with appropriate coordinates using the 'display_image' method\n",
    "waxs_analysis.display_image(smoothed_image, \n",
    "                            title='Smoothed Image', \n",
    "                            cmap='jet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Signal-to-Noise Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_image = waxs_analysis.calculate_SNR(smoothed_image)\n",
    "# waxs_analysis.snrtemp\n",
    "waxs_analysis.reciptiff_xr.SNR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Class to Compare Simulated (.int) Files to Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def gaussian(x, A, mu, sigma):\n",
    "    return A * np.exp(-(x - mu)**2 / (2 * sigma**2))\n",
    "\n",
    "def lorentzian(x, A, mu, gamma):\n",
    "    return A * (1 / np.pi) * (gamma / ((x - mu)**2 + gamma**2))\n",
    "\n",
    "def pseudo_voigt(x, A, mu, sigma, gamma, eta):\n",
    "    return eta * gaussian(x, A, mu, sigma) + (1 - eta) * lorentzian(x, A, mu, gamma)\n",
    "\n",
    "class TraceComparison:\n",
    "    def __init__(self, trace1, trace2):\n",
    "        self.trace1 = trace1\n",
    "        self.trace2 = trace2\n",
    "        self.background_params = None\n",
    "        self.optimized_background = pd.Series(dtype='float64')\n",
    "    \n",
    "    def normalize_to_max(self):\n",
    "        max1 = self.trace1['intensity'].max()\n",
    "        max2 = self.trace2['intensity'].max()\n",
    "        self.trace1.loc[:, 'intensity'] /= max1\n",
    "        self.trace2.loc[:, 'intensity'] /= max2\n",
    "\n",
    "    def normalize_to_baseline(self, q_min, q_max):\n",
    "        baseline1 = self.trace1[(self.trace1['q'] >= q_min) & (self.trace1['q'] <= q_max)]['intensity'].mean()\n",
    "        baseline2 = self.trace2[(self.trace2['q'] >= q_min) & (self.trace2['q'] <= q_max)]['intensity'].mean()\n",
    "        self.trace1['intensity'] /= baseline1\n",
    "        self.trace2['intensity'] /= baseline2\n",
    "\n",
    "    def find_peaks(self, height, distance):\n",
    "        self.peaks1, _ = find_peaks(self.trace1['intensity'], height=height, distance=distance)\n",
    "        self.peaks2, _ = find_peaks(self.trace2['intensity'], height=height, distance=distance)\n",
    "\n",
    "    def compare_peak_widths(self):\n",
    "        self.widths1 = self.trace1.iloc[self.peaks1]['q'].values\n",
    "        self.widths2 = self.trace2.iloc[self.peaks2]['q'].values\n",
    "\n",
    "    def calculate_residual(self):\n",
    "        # Create interpolation functions for the intensity values based on 'q'\n",
    "        f1 = interp1d(self.trace1['q'], self.trace1['intensity'], kind='linear', fill_value=\"extrapolate\")\n",
    "        f2 = interp1d(self.trace2['q'], self.trace2['intensity'], kind='linear', fill_value=\"extrapolate\")\n",
    "\n",
    "        # Create a common 'q' range\n",
    "        common_q = np.linspace(max(self.trace1['q'].min(), self.trace2['q'].min()),\n",
    "                            min(self.trace1['q'].max(), self.trace2['q'].max()),\n",
    "                            num=max(len(self.trace1['q']), len(self.trace2['q'])))\n",
    "\n",
    "        # Interpolate the intensity values to the common 'q' range\n",
    "        common_intensity1 = f1(common_q)\n",
    "        common_intensity2 = f2(common_q)\n",
    "\n",
    "        # Calculate the residual\n",
    "        self.residual = pd.Series(common_intensity1 - common_intensity2, index=common_q, name='residual')\n",
    "\n",
    "    def minimize_residual(self):\n",
    "        min_q = max(self.trace1['q'].min(), self.trace2['q'].min())\n",
    "        max_q = min(self.trace1['q'].max(), self.trace2['q'].max())\n",
    "        common_range = (self.trace1['q'] >= min_q) & (self.trace1['q'] <= max_q)\n",
    "        \n",
    "        x = self.trace1.loc[common_range, 'q']\n",
    "        y1 = self.trace1.loc[common_range, 'intensity']\n",
    "        y2 = self.trace2.loc[common_range, 'intensity']\n",
    "\n",
    "        params, _ = curve_fit(pseudo_voigt, x, y1 - y2)\n",
    "        self.fitted_curve = pseudo_voigt(x, *params)\n",
    "        self.residual = y1 - (y2 + self.fitted_curve)\n",
    "\n",
    "    def optimize_background(self):\n",
    "        def objective(params):\n",
    "            noise_level, slope, intercept = params\n",
    "            background = noise_level + slope * self.trace2['q'] + intercept\n",
    "            modified_trace2 = self.trace2['intensity'] + background\n",
    "            f1 = interp1d(self.trace1['q'], self.trace1['intensity'], kind='linear', fill_value=\"extrapolate\")\n",
    "            f2 = interp1d(self.trace2['q'], modified_trace2, kind='linear', fill_value=\"extrapolate\")\n",
    "\n",
    "            # Define a common 'q' range\n",
    "            common_q = np.linspace(max(self.trace1['q'].min(), self.trace2['q'].min()),\n",
    "                                   min(self.trace1['q'].max(), self.trace2['q'].max()),\n",
    "                                   num=max(len(self.trace1['q']), len(self.trace2['q'])))\n",
    "\n",
    "            residual = f1(common_q) - f2(common_q)\n",
    "            return np.sum(residual ** 2)\n",
    "\n",
    "        initial_guess = [0, 0, 0] if self.background_params is None else self.background_params\n",
    "        result = minimize(objective, initial_guess, method='L-BFGS-B')\n",
    "        self.background_params = result.x\n",
    "\n",
    "        # Apply the optimized background to the trace intensities\n",
    "        self.optimized_background = self.background_params[0] + self.background_params[1] * self.trace2['q'] + self.background_params[2]\n",
    "        self.trace2.loc[:, 'intensity'] += self.optimized_background\n",
    "        self.trace2.loc[:, 'background'] = self.optimized_background  # Store the background in the DataFrame\n",
    "\n",
    "        self.calculate_residual()\n",
    "        \n",
    "    def plot_normalization(self):\n",
    "        plt.figure()\n",
    "        plt.plot(self.trace1['q'], self.trace1['intensity'], label='Trace 1')\n",
    "        plt.plot(self.trace2['q'], self.trace2['intensity'], label='Trace 2')\n",
    "        plt.title('Normalized Traces')\n",
    "        plt.xlabel('q')\n",
    "        plt.ylabel('Intensity')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_peaks(self):\n",
    "        plt.figure()\n",
    "        plt.plot(self.trace1['q'], self.trace1['intensity'], label='Trace 1')\n",
    "        plt.plot(self.trace1['q'].iloc[self.peaks1], self.trace1['intensity'].iloc[self.peaks1], 'x', label='Peaks 1')\n",
    "        plt.plot(self.trace2['q'], self.trace2['intensity'], label='Trace 2')\n",
    "        plt.plot(self.trace2['q'].iloc[self.peaks2], self.trace2['intensity'].iloc[self.peaks2], 'o', label='Peaks 2')\n",
    "        plt.title('Peak Positions')\n",
    "        plt.xlabel('q')\n",
    "        plt.ylabel('Intensity')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_residual(self):\n",
    "        plt.figure()\n",
    "        plt.plot(self.trace1.loc[self.residual.index, 'q'], self.residual, label='Residual')\n",
    "        plt.title('Residual Between Traces')\n",
    "        plt.xlabel('q')\n",
    "        plt.ylabel('Intensity Difference')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_minimization(self):\n",
    "        plt.figure()\n",
    "        plt.plot(self.trace1['q'], self.residual + self.fitted_curve, label='Fitted Curve')\n",
    "        plt.plot(self.trace1['q'], self.residual, label='Original Residual')\n",
    "        plt.title('Minimized Residual')\n",
    "        plt.xlabel('q')\n",
    "        plt.ylabel('Intensity Difference')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def save_traces_to_csv(self, trace1_filename, trace2_filename, savepath=None):\n",
    "        if savepath:\n",
    "            savepath = Path(savepath)\n",
    "            savepath.mkdir(parents=True, exist_ok=True)\n",
    "            trace1_filepath = savepath / trace1_filename\n",
    "            trace2_filepath = savepath / trace2_filename\n",
    "        else:\n",
    "            trace1_filepath = trace1_filename\n",
    "            trace2_filepath = trace2_filename\n",
    "        \n",
    "        self.trace1.to_csv(trace1_filepath, index=False)\n",
    "        self.trace2.to_csv(trace2_filepath, index=False)\n",
    "\n",
    "    @classmethod\n",
    "    def load_and_plot_traces(cls, trace1_path, trace2_path, savepath=None):\n",
    "        trace1 = pd.read_csv(trace1_path)\n",
    "        trace2 = pd.read_csv(trace2_path)\n",
    "\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(trace1['q'], trace1['intensity'], label='Trace 1')\n",
    "        plt.plot(trace2['q'], trace2['intensity'], label='Trace 2')\n",
    "        plt.xlabel('q')\n",
    "        plt.ylabel('Intensity')\n",
    "        plt.legend()\n",
    "        plt.title('Loaded Traces')\n",
    "        \n",
    "        if savepath:\n",
    "            savepath = Path(savepath)\n",
    "            plt.savefig(savepath)\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TraceComparison object\n",
    "bbox_df = pd.DataFrame({'q': waxs_analysis.integrate1d_da.qr.values, 'intensity': waxs_analysis.integrate1d_da.values})\n",
    "\n",
    "# Filter out data below q-value of 0.5\n",
    "bbox_df = bbox_df[bbox_df['q'] >= 0.5]\n",
    "filtered_data = filtered_data[filtered_data['q'] >= 0.5]\n",
    "\n",
    "# Initialize the TraceComparison object and perform calculations\n",
    "comparison = TraceComparison(bbox_df, filtered_data)\n",
    "comparison.normalize_to_max()\n",
    "comparison.calculate_residual()\n",
    "\n",
    "# Plot initial comparison and residual\n",
    "plt.figure(figsize=(6, 10))\n",
    "\n",
    "# Plot the normalized traces\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.plot(comparison.trace1['q'], comparison.trace1['intensity'], label='pygix bbox (Normalized)', color='b')\n",
    "plt.plot(comparison.trace2['q'], comparison.trace2['intensity'], label='Loaded .int File (Normalized)', color='g')\n",
    "plt.title('Initial Comparison of Traces')\n",
    "plt.xlabel('q')\n",
    "plt.ylabel('Intensity')\n",
    "plt.legend()\n",
    "\n",
    "# Plot the initial residual\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.plot(comparison.residual.index, comparison.residual.values, label='Initial Residual', color='r')\n",
    "plt.title('Initial Residual')\n",
    "plt.xlabel('q')\n",
    "plt.ylabel('Intensity Difference')\n",
    "plt.legend()\n",
    "\n",
    "# Optimize the background\n",
    "comparison.optimize_background()\n",
    "\n",
    "# Plot the normalized traces with optimized background\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.plot(comparison.trace1['q'], comparison.trace1['intensity'], label='pygix bbox (Normalized)', color='b')\n",
    "plt.plot(comparison.trace2['q'], comparison.trace2['intensity'], label='Loaded .int File (Normalized)', color='g')\n",
    "plt.title('Comparison of Traces (With Optimized Background)')\n",
    "plt.xlabel('q')\n",
    "plt.ylabel('Intensity')\n",
    "plt.legend()\n",
    "\n",
    "# Plot the new residual\n",
    "plt.subplot(4, 1, 4)\n",
    "plt.plot(comparison.residual.index, comparison.residual.values, label='New Residual', color='r')\n",
    "plt.title('New Residual')\n",
    "plt.xlabel('q')\n",
    "plt.ylabel('Intensity Difference')\n",
    "plt.legend()\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Initialize the TraceComparison object and perform calculations\n",
    "# comparison = TraceComparison(bbox_df, filtered_data)\n",
    "# comparison.normalize_to_max()\n",
    "# comparison.calculate_residual()\n",
    "\n",
    "# # Save the traces for debugging\n",
    "# comparison.save_traces_to_csv('trace1.csv', 'trace2.csv', savepath=savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct file paths\n",
    "trace1_path = Path('/Users/keithwhite/github_repositories/pyWAXS/examples_local/projects/LaB6_SRM660c_Calib/analysis/simulation/trace1_saved.csv')\n",
    "trace2_path = Path('/Users/keithwhite/github_repositories/pyWAXS/examples_local/projects/LaB6_SRM660c_Calib/analysis/simulation/trace2_saved.csv')\n",
    "\n",
    "# Load and plot the traces\n",
    "TraceComparison.load_and_plot_traces(trace1_path, trace2_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Developing Image Stitching Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required module imports for the ImageStitching class\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from skimage import io, feature, color\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import corner_harris, corner_peaks\n",
    "import cv2\n",
    "\n",
    "class ImageStitching:\n",
    "    def __init__(self, files: List[Path]):\n",
    "        self.files = files\n",
    "        self.images_original = []\n",
    "        self.images_gray = []\n",
    "        self.keypoints_list = []\n",
    "        self.load_and_preprocess_images()\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_to_8bit(image):\n",
    "        image_norm = (image - np.min(image)) / (np.max(image) - np.min(image))\n",
    "        return (image_norm * 255).astype(np.uint8)\n",
    "    \n",
    "    def load_and_preprocess_images(self):\n",
    "        for file in self.files:\n",
    "            image_original = io.imread(file)\n",
    "            self.images_original.append(image_original)\n",
    "            \n",
    "            if len(image_original.shape) == 3:\n",
    "                image_gray = color.rgb2gray(image_original)\n",
    "            else:\n",
    "                image_gray = image_original\n",
    "            \n",
    "            image_gray = self.convert_to_8bit(image_gray)\n",
    "            self.images_gray.append(image_gray)\n",
    "\n",
    "    def find_sift_keypoints(self):\n",
    "        self.keypoints_list = []\n",
    "        for i, image_gray in enumerate(self.images_gray):\n",
    "            keypoints, _ = self.detect_features(image_gray)\n",
    "            keypoints = np.array([kp.pt for kp in keypoints])\n",
    "            self.keypoints_list.append(keypoints)\n",
    "\n",
    "    @staticmethod\n",
    "    def detect_features(image):\n",
    "        sift = cv2.SIFT_create()\n",
    "        return sift.detectAndCompute(image, None)\n",
    "    \n",
    "    '''\n",
    "    def display_keypoints(self, index: int, title='Image', cmap='turbo'):\n",
    "        img_values = self.images_gray[index]\n",
    "        keypoints = self.keypoints_list[index]\n",
    "        \n",
    "        vmin = np.nanpercentile(img_values, 10)\n",
    "        vmax = np.nanpercentile(img_values, 99)\n",
    "    \n",
    "        plt.imshow(np.flipud(img_values),\n",
    "                   cmap=cmap,\n",
    "                   vmin=vmin,\n",
    "                   vmax=vmax,\n",
    "                   aspect='auto')\n",
    "        \n",
    "        # Draw keypoints on top of the image\n",
    "        for keypoint in keypoints:\n",
    "            x, y = keypoint\n",
    "            plt.scatter(x, y, s=10, c='red', marker='o')\n",
    "        \n",
    "        plt.title(title)\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "    '''\n",
    "    \n",
    "    def display_keypoints(self, index: int, title='Image', cmap='turbo'):\n",
    "        # plt.close('all')\n",
    "        img_values = self.images_gray[index]\n",
    "        keypoints = self.keypoints_list[index]\n",
    "        \n",
    "        vmin = np.nanpercentile(img_values, 10)\n",
    "        vmax = np.nanpercentile(img_values, 99)\n",
    "    \n",
    "        plt.imshow(np.flipud(img_values),\n",
    "                   cmap=cmap,\n",
    "                   vmin=vmin,\n",
    "                   vmax=vmax,\n",
    "                   aspect='auto')\n",
    "        \n",
    "        # Draw keypoints on top of the image\n",
    "        for keypoint in keypoints:\n",
    "            x, y = keypoint\n",
    "            plt.scatter(x, y, s=10, c='red', marker='o')\n",
    "        \n",
    "        plt.title(title)\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "    \n",
    "    def find_keypoints(self, n_keypoints: int = 5000, fast_threshold: float = 0.01):\n",
    "        for image_gray in self.images_gray:\n",
    "            orb = feature.ORB(n_keypoints=n_keypoints, fast_threshold=fast_threshold)\n",
    "            orb.detect_and_extract(image_gray)\n",
    "            self.keypoints_list.append(orb.keypoints)\n",
    "\n",
    "    def plot_original(self, index: int):\n",
    "        image = self.images_original[index]\n",
    "        vmin, vmax = np.nanpercentile(image, [10, 99])\n",
    "        plt.imshow(image, cmap='turbo', vmin=vmin, vmax=vmax)\n",
    "        plt.title(\"Original Image\")\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_keypoints(self, index: int):\n",
    "        image = self.images_original[index]\n",
    "        keypoints = self.keypoints_list[index]\n",
    "        vmin, vmax = np.nanpercentile(image, [10, 99])\n",
    "        plt.imshow(image, cmap='turbo', vmin=vmin, vmax=vmax)\n",
    "        plt.plot(keypoints[:, 1], keypoints[:, 0], 'r.', markersize=5)\n",
    "        plt.title(\"Keypoints\")\n",
    "        plt.show()\n",
    "        \n",
    "    def find_harris_keypoints(self, min_distance: int = 5):\n",
    "        self.keypoints_list = []\n",
    "        for image_gray in self.images_gray:\n",
    "            harris_response = corner_harris(image_gray)\n",
    "            keypoints = corner_peaks(harris_response, min_distance=min_distance)\n",
    "            self.keypoints_list.append(keypoints)\n",
    "\n",
    "    def plot_original_with_keypoints(self, index: int):\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "        \n",
    "        # Original Image\n",
    "        image = self.images_original[index]\n",
    "        vmin, vmax = np.nanpercentile(image, [10, 99])\n",
    "        axs[0].imshow(image, cmap='turbo', vmin=vmin, vmax=vmax)\n",
    "        axs[0].set_title(\"Original Image\")\n",
    "        \n",
    "        # Keypoints\n",
    "        keypoints = self.keypoints_list[index]\n",
    "        axs[1].imshow(image, cmap='turbo', vmin=vmin, vmax=vmax)\n",
    "        axs[1].plot(keypoints[:, 1], keypoints[:, 0], 'r.', markersize=5)\n",
    "        axs[1].set_title(\"Keypoints\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_sift_keypoints(self, index: int):\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "        \n",
    "        # Original Image\n",
    "        image = self.images_original[index]\n",
    "        vmin, vmax = np.nanpercentile(image, [10, 99])\n",
    "        axs[0].imshow(image, cmap='turbo', vmin=vmin, vmax=vmax)\n",
    "        axs[0].set_title(\"Original Image\")\n",
    "        \n",
    "        # SIFT Keypoints\n",
    "        keypoints = self.keypoints_list[index]\n",
    "        print(f\"Plotting Image {index}: Keypoints shape: {keypoints.shape}\")  # Add debug print\n",
    "        axs[1].imshow(image, cmap='turbo', vmin=vmin, vmax=vmax)\n",
    "        # axs[1].plot(keypoints[:, 0], keypoints[:, 1], 'r.', markersize=5)  # Corrected indexing\n",
    "        axs[1].plot(keypoints[:, 1], keypoints[:, 0], 'r.', markersize=5)\n",
    "        axs[1].set_title(\"SIFT Keypoints\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Initialize the class and find keypoints with local file paths for the example\n",
    "files = [\n",
    "    Path(\"/Users/keithwhite/github_repositories/pyWAXS/examples_local/projects/1MAI_3PbI2_DMF_1M_jun23/stitch/sam16_3mai1pbi2_dmf_1m_5scfh_Si_40uL_010_2356.6s_x-0.001_th0.300_10.00s_1116233_maxs.tiff\"),\n",
    "    Path(\"/Users/keithwhite/github_repositories/pyWAXS/examples_local/projects/1MAI_3PbI2_DMF_1M_jun23/stitch/sam16_3mai1pbi2_dmf_1m_5scfh_Si_40uL_010_2461.2s_x-0.001_th0.300_10.00s_1116234_maxs.tiff\")\n",
    "]\n",
    "\n",
    "stitcher_sift = ImageStitching(files)\n",
    "\n",
    "plt.close('all')\n",
    "\n",
    "# Find and visualize SIFT keypoints\n",
    "stitcher_sift.find_sift_keypoints()\n",
    "stitcher_sift.display_keypoints(0, title='Image 1')\n",
    "# stitcher_sift.display_keypoints(1, title='Image 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stitcher_sift.display_keypoints(1, title='Image 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if xfeatures2d module is available\n",
    "sift_available = \"xfeatures2d\" in cv2.__dict__\n",
    "if sift_available:\n",
    "    print(\"SIFT is available in OpenCV.\")\n",
    "else:\n",
    "    print(\"SIFT is not available in OpenCV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the class and find keypoints with local file paths for the example\n",
    "files = [\n",
    "    Path(\"/Users/keithwhite/github_repositories/pyWAXS/examples_local/projects/1MAI_3PbI2_DMF_1M_jun23/stitch/sam16_3mai1pbi2_dmf_1m_5scfh_Si_40uL_010_2356.6s_x-0.001_th0.300_10.00s_1116233_maxs.tiff\"),\n",
    "    Path(\"/Users/keithwhite/github_repositories/pyWAXS/examples_local/projects/1MAI_3PbI2_DMF_1M_jun23/stitch/sam16_3mai1pbi2_dmf_1m_5scfh_Si_40uL_010_2461.2s_x-0.001_th0.300_10.00s_1116234_maxs.tiff\")\n",
    "]\n",
    "\n",
    "# Initialize the class and find SIFT keypoints\n",
    "stitcher_sift = ImageStitching(files)\n",
    "stitcher_sift.find_sift_keypoints()\n",
    "\n",
    "# Test the new plotting method for SIFT keypoints\n",
    "stitcher_sift.plot_sift_keypoints(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the class and find keypoints with local file paths for the example\n",
    "files = [\n",
    "    Path(\"/Users/keithwhite/github_repositories/pyWAXS/examples_local/projects/1MAI_3PbI2_DMF_1M_jun23/stitch/sam16_3mai1pbi2_dmf_1m_5scfh_Si_40uL_010_2356.6s_x-0.001_th0.300_10.00s_1116233_maxs.tiff\"),\n",
    "    Path(\"/Users/keithwhite/github_repositories/pyWAXS/examples_local/projects/1MAI_3PbI2_DMF_1M_jun23/stitch/sam16_3mai1pbi2_dmf_1m_5scfh_Si_40uL_010_2461.2s_x-0.001_th0.300_10.00s_1116234_maxs.tiff\")\n",
    "]\n",
    "\n",
    "# Initialize the class and find SIFT keypoints\n",
    "stitcher_sift = ImageStitching(files)\n",
    "stitcher_sift.find_sift_keypoints()\n",
    "\n",
    "# Test the new plotting method for SIFT keypoints\n",
    "stitcher_sift.plot_sift_keypoints(0)\n",
    "\n",
    "# Example usage\n",
    "# stitcher_example = ImageStitching(local_files)\n",
    "# stitcher_example.find_keypoints()\n",
    "\n",
    "# Note: The actual images won't be loaded in this environment due to path constraints. This is just for demonstration.\n",
    "# stitcher_example.files\n",
    "# Re-initialize the class and find keypoints with the initially uploaded files (as local files are not accessible here)\n",
    "\n",
    "# Test the plotting methods\n",
    "# stitcher_example.plot_original(0)\n",
    "# stitcher_example.plot_keypoints(0)\n",
    "# stitcher_example.plot_original_with_keypoints(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Processer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the required imports for the class\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Modify the ImageProcessor class to match the given contrast scaling method\n",
    "class ImageProcessor:\n",
    "    @staticmethod\n",
    "    def convert_to_8bit(image):\n",
    "        image_norm = (image - np.min(image)) / (np.max(image) - np.min(image))\n",
    "        return (image_norm * 255).astype(np.uint8)\n",
    "    \n",
    "    @staticmethod\n",
    "    def detect_features(image):\n",
    "        sift = cv2.SIFT_create()\n",
    "        return sift.detectAndCompute(image, None)\n",
    "    \n",
    "    @staticmethod\n",
    "    def display_image(img, ax, title='Image', cmap='turbo'):\n",
    "        if img is None or not isinstance(img, np.ndarray):\n",
    "            raise ValueError(\"The input image is None or not of a compatible type.\")\n",
    "        \n",
    "        img_values = img\n",
    "        if np.all(np.isnan(img_values)) or img_values.size == 0:\n",
    "            raise ValueError(\"The input image is empty or contains only NaN values.\")\n",
    "        \n",
    "        vmin = np.nanpercentile(img_values, 10)\n",
    "        vmax = np.nanpercentile(img_values, 99)\n",
    "        ax.imshow(np.flipud(img_values),\n",
    "                   cmap=cmap,\n",
    "                   vmin=vmin,\n",
    "                   vmax=vmax,\n",
    "                   aspect='auto')\n",
    "        ax.set_title(title)\n",
    "        ax.axis('off')\n",
    "        \n",
    "    def process_and_display_single_image(self, image, title='Image'):\n",
    "        # Convert to 8-bit grayscale\n",
    "        image_8bit = self.convert_to_8bit(image)\n",
    "        \n",
    "        # Detect features (keypoints)\n",
    "        keypoints, _ = self.detect_features(image_8bit)\n",
    "        \n",
    "        # Create a figure with 2 subplots\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "        \n",
    "        # Display the original image in the first subplot\n",
    "        self.display_image(image_8bit, axes[0], title=f'{title} - Original')\n",
    "        \n",
    "        # Display the keypoints in the second subplot\n",
    "        img_keypoints = cv2.drawKeypoints(image_8bit, keypoints, None)\n",
    "        self.display_image(img_keypoints, axes[1], title=f'{title} - {len(keypoints)} Keypoints')\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "# Instantiate the ImageProcessor class\n",
    "processor = ImageProcessor()\n",
    "\n",
    "# Process and display each image one by one\n",
    "for i, img in enumerate(images):\n",
    "    processor.process_and_display_single_image(img, title=f'Image {i+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the ImageProcessor class\n",
    "processor = ImageProcessor()\n",
    "\n",
    "# Process and display each image one by one\n",
    "for i, img in enumerate(images):\n",
    "    processor.process_and_display_single_image(img, title=f'Image {i+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder_structure(base_path, folder_name):\n",
    "    main_folder = os.path.join(base_path, folder_name)\n",
    "    \n",
    "    # Check if folder exists\n",
    "    if not os.path.exists(main_folder):\n",
    "        os.makedirs(main_folder)\n",
    "    \n",
    "    # Create additional folders\n",
    "    subfolders = ['poni', 'mask', 'data', 'analysis', 'poscar', 'stitch']\n",
    "    subsubfolders = ['hdf5', 'png', 'simulation']\n",
    "\n",
    "    for sub in subfolders:\n",
    "        os.makedirs(os.path.join(main_folder, sub), exist_ok=True)\n",
    "        if sub == 'analysis':\n",
    "            analysisPath = Path(main_folder).joinpath('analysis') # Generalized for file outputs from analysis\n",
    "            for subsub in subsubfolders:\n",
    "                os.makedirs(os.path.join(analysisPath, subsub), exist_ok=True)\n",
    "\n",
    "def update_file_path(folder_path: Path, extension: str) -> Optional[Path]:\n",
    "    files = list(folder_path.glob(f'*.{extension}'))\n",
    "\n",
    "    if len(files) > 1:\n",
    "        raise ValueError(f\"Multiple .{extension} files found in the folder.\")\n",
    "    elif len(files) == 0:\n",
    "        raise ValueError(f\"No .{extension} files found in the folder.\")\n",
    "    \n",
    "    return folder_path.joinpath(files[0].name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyGIXS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
