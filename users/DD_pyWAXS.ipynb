{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class & Module Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- Module WAXS Classes -- ##\n",
    "from WAXSTransform import WAXSTransform\n",
    "from WAXSReduce import WAXSReduce\n",
    "from WAXSReduce import ImageInterpolator\n",
    "from WAXSReduce import Integration1D\n",
    "from WAXSReduce import WAXSTOPAS\n",
    "from WAXSReduce import Azimuth1D\n",
    "from WAXSReduce import ImageTransform\n",
    "from WAXSSearch import WAXSSearch\n",
    "from WAXSDiffSim import WAXSDiffSim\n",
    "\n",
    "# - Import Relevant Modules\"\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pathlib, os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math, re, os\n",
    "from typing import Optional, Union\n",
    "from shutil import copy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Data Folder Function\n",
    "#### Load Pandas Dataframe from Spreadsheet - Find Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_int_file(file_path: Path, wavelength: float) -> pd.DataFrame:\n",
    "    # Read the file, skipping the first two lines\n",
    "    df = pd.read_csv(file_path, skiprows=2, header=None, delimiter='\\s+')\n",
    "    \n",
    "    # Rename the columns\n",
    "    df.columns = ['twotheta', 'intensity', 'error']\n",
    "    \n",
    "    # Calculate q based on twotheta and wavelength\n",
    "    df['q'] = (4 * math.pi / wavelength) * np.sin(np.radians(df['twotheta'] / 2))\n",
    "    \n",
    "    # Add headers for wavelength and file path\n",
    "    df.attrs['wavelength'] = wavelength\n",
    "    df.attrs['file_path'] = str(file_path)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_folder_structure(base_path: Path, folder_name: str):\n",
    "    \"\"\"\n",
    "    Creates the main folder and its subfolders based on the provided base path and folder name.\n",
    "    \"\"\"\n",
    "    main_folder = base_path.joinpath(folder_name)\n",
    "    \n",
    "    # Create main folder if it doesn't exist\n",
    "    main_folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Define additional subfolders and subsubfolders\n",
    "    subfolders = ['poni', 'mask', 'data', 'analysis', 'poscar', 'stitch']\n",
    "    subsubfolders = ['hdf5', 'png', 'simulation']\n",
    "\n",
    "    # Create subfolders\n",
    "    for sub in subfolders:\n",
    "        subfolder_path = main_folder.joinpath(sub)\n",
    "        subfolder_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Create subsubfolders under 'analysis'\n",
    "        if sub == 'analysis':\n",
    "            for subsub in subsubfolders:\n",
    "                subsubfolder_path = subfolder_path.joinpath(subsub)\n",
    "                subsubfolder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def create_project_folders(project_input: Union[str, Path], base_path: Path):\n",
    "    \"\"\"\n",
    "    Generalizes the folder and subfolder creation process for multiple project names.\n",
    "    Project names are either specified individually or extracted from an Excel file's 'Project Folder Name' column.\n",
    "    \"\"\"\n",
    "    # Handle case where project_input is an Excel file\n",
    "    if isinstance(project_input, Path):\n",
    "        df = pd.read_excel(project_input)\n",
    "        project_names = df['Project Folder Name'].unique()\n",
    "    else:\n",
    "        project_names = [project_input]\n",
    "    \n",
    "    # Create folder structure for each unique project name\n",
    "    for project_name in project_names:\n",
    "        create_folder_structure(base_path, project_name)\n",
    "\n",
    "def populate_tiff_details_in_excel(excel_path: Path, tiff_folder_path: Path, output_csv_path: Path):\n",
    "    \"\"\"\n",
    "    Reads an Excel file and populates the 'scanID', 'filename', and 'filepath' fields based on the .tiff files in a folder.\n",
    "    \n",
    "    Parameters:\n",
    "    - excel_path: Path to the input Excel file\n",
    "    - tiff_folder_path: Path to the folder containing the .tiff files\n",
    "    - output_csv_path: Path to save the updated DataFrame as a CSV\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with updated fields\n",
    "    \"\"\"\n",
    "    # Read Excel into DataFrame\n",
    "    df = pd.read_excel(excel_path)\n",
    "    \n",
    "    # Initialize empty lists to hold the new values\n",
    "    new_scanIDs = []\n",
    "    new_filenames = []\n",
    "    new_filepaths = []\n",
    "    \n",
    "    # Loop through each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        # Extract necessary values from each row\n",
    "        filename_prefix = str(row['filename_prefix'])\n",
    "        substrateID = str(row['substrateID'])\n",
    "        position = str(row['position'])\n",
    "        incidence_angle = str(row['incidence_angle'])\n",
    "        exptime = str(row['exptime'])\n",
    "\n",
    "        # Construct the search prefix\n",
    "        search_prefix = f\"{filename_prefix}{substrateID}_{position}_{incidence_angle}_{exptime}\"\n",
    "        # print (search_prefix)\n",
    "        # Initialize variables to hold the new values for this row\n",
    "        new_scanID = ''\n",
    "        new_filename = ''\n",
    "        new_filepath = ''\n",
    "        \n",
    "        # Loop through each .tiff file in the folder and check if it starts with the search prefix\n",
    "        for tiff_file in tiff_folder_path.glob(\"*.tiff\"):\n",
    "            if tiff_file.name.startswith(search_prefix):\n",
    "                # Extract details from the filename\n",
    "                details = tiff_file.name[len(search_prefix):].split('_')\n",
    "                new_scanID = details[1]\n",
    "                new_filename = tiff_file.name\n",
    "                new_filepath = str(tiff_file)\n",
    "                break\n",
    "        \n",
    "        # Append the new values to the lists\n",
    "        new_scanIDs.append(new_scanID)\n",
    "        new_filenames.append(new_filename)\n",
    "        new_filepaths.append(new_filepath)\n",
    "        \n",
    "    # Update the DataFrame with the new values\n",
    "    df['scanID'] = new_scanIDs\n",
    "    df['filename'] = new_filenames\n",
    "    df['filepath'] = new_filepaths\n",
    "    \n",
    "    # Save the updated DataFrame as a CSV\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def copy_tiff_files_to_project_folders(df: pd.DataFrame, base_path: Path):\n",
    "    \"\"\"\n",
    "    Copies .tiff files to their respective project folders based on the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame with the 'filepath' and 'projectname' fields\n",
    "    - base_path: Base path where the project folders are located\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    for index, row in df.iterrows():\n",
    "        src_filepath = Path(row['filepath'])\n",
    "        project_folder_name = row['projectname']\n",
    "        if src_filepath.exists():\n",
    "            dest_folder = base_path.joinpath(project_folder_name, 'data')\n",
    "            dest_folder.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            dest_filepath = dest_folder.joinpath(src_filepath.name)\n",
    "            copy2(src_filepath, dest_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_path = Path(\"/Users/keithwhite/github_repositories/pyWAXS/examples_local/projects/231012_demisamples.xlsx\")\n",
    "basePath = Path(\"/Users/keithwhite/github_repositories/pyWAXS/examples_local/projects\")\n",
    "tiff_folder_path = Path(\"/Users/keithwhite/Desktop/activedata/KWhite4/waxs/raw\")\n",
    "output_csv_path = '/Users/keithwhite/github_repositories/pyWAXS/examples_local/projects/updated.csv'\n",
    "\n",
    "# Run the function\n",
    "updated_df = populate_tiff_details_in_excel(excel_path, tiff_folder_path, output_csv_path)\n",
    "updated_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy Files to Project Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy_tiff_files_to_project_folders(updated_df, basePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "\n",
    "## -- CHANGE THE PROJECT NAME & ScanID -- ##\n",
    "# projectname = 'AgBh_Calib'\n",
    "\n",
    "# projectname = '002-D1_1-An-C1_DMF'\n",
    "# scanID_to_use = \"681925\"  # Replace with the scanID you want to use\n",
    "\n",
    "# projectname = '002-D8_1-An-C3_6-1'\n",
    "# scanID_to_use =  \"682441\" #\"682436\"\n",
    "\n",
    "projectname = '002-D11_2-An-C2_DMF'\n",
    "scanID_to_use =  \"682957\"\n",
    "\n",
    "# -- Base Path Definitions -- #\n",
    "basePath = pathlib.Path('/Users/keithwhite/github_repositories/pyWAXS/examples_local/projects')\n",
    "create_folder_structure(basePath, projectname)\n",
    "projectPath = basePath.joinpath(projectname)\n",
    "\n",
    "# -- Project Path Definitions -- #\n",
    "dataPath = projectPath.joinpath('data')  # Generalized path for data.\n",
    "poniPath = projectPath.joinpath('poni')  # PONI (.poni) File\n",
    "analysisPath = projectPath.joinpath('analysis')  # Generalized for file outputs from analysis\n",
    "maskPath = projectPath.joinpath('mask')  # MASK (.edf, .json)\n",
    "poscarPath = projectPath.joinpath('poscar')\n",
    "\n",
    "# -- Analysis Subpaths -- #\n",
    "hdf5Path = analysisPath.joinpath('hdf5')  # output path for generated hdf5 files\n",
    "pngPath = analysisPath.joinpath('png')\n",
    "simulationPath = analysisPath.joinpath('simulation')\n",
    "\n",
    "def update_file_path(folder_path: Path, extension: str, scanID: Optional[str] = None) -> Optional[Path]:\n",
    "    files = list(folder_path.glob(f'*.{extension}'))\n",
    "\n",
    "    if scanID:\n",
    "        files = [f for f in files if f\"_{scanID}_\" in f.name]\n",
    "\n",
    "    if len(files) > 1:\n",
    "        raise ValueError(f\"Multiple .{extension} files found in the folder for scanID {scanID}.\")\n",
    "    elif len(files) == 0:\n",
    "        raise ValueError(f\"No .{extension} files found in the folder for scanID {scanID}.\")\n",
    "    \n",
    "    return folder_path.joinpath(files[0].name)\n",
    "\n",
    "## --- Select File Here -- ##\n",
    "try:\n",
    "    # scanID_to_use = \"681\"  # Replace with the scanID you want to use\n",
    "    dataPath = update_file_path(dataPath, 'tiff', scanID=scanID_to_use)\n",
    "    maskPath = update_file_path(maskPath, 'edf')\n",
    "    poniPath = update_file_path(poniPath, 'poni')\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "\n",
    "print(f\"Updated dataPath: {dataPath}\")\n",
    "print(f\"Updated poniPath: {poniPath}\")\n",
    "print(f\"Updated maskPath: {maskPath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Metadata Keylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_keylist = ['damaradayton', # Damara Dayton\n",
    "                    'substrateID', # Substrate ID\n",
    "                    'detector_position', # Detector Position\n",
    "                    'incident_angle', # Incidence Angle\n",
    "                    'exptime', # Exposure Time\n",
    "                    'scanID', # Scan ID\n",
    "                    'detext'] # Extension of the detector identifier at BNL 'maxs' v. 'saxs' v, 'waxs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a WAXSReduce Project Instance Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - WAXSReduce Class Instantiation: Load the data, apply mask, calculate and map q-range, Ewald sphere corrections/pixel splitting, and caked pixel splitting.\n",
    "waxs_analysis = WAXSReduce(poniPath=poniPath, \n",
    "        maskPath=maskPath, \n",
    "        tiffPath=dataPath, \n",
    "        metadata_keylist=metadata_keylist,\n",
    "        energy = 12.7)\n",
    "\n",
    "methods = 'np', 'cython', 'bbox', 'splitpix', 'lut'\n",
    "# Run the 1D integration using the run_pg_integrate1D method.\n",
    "waxs_analysis.run_pg_integrate1D(npt=1024, \n",
    "                                 method='bbox', # \n",
    "                                 correctSolidAngle=True, \n",
    "                                 polarization_factor=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the Reciprocal Space Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "data = waxs_analysis.reciptiff_xr\n",
    "# data = waxs_analysis.fold_image(data, 'chi')\n",
    "data, (max_x, max_y) = waxs_analysis.normalize_image(img = data, \n",
    "                                                  normalizerecip=False)\n",
    "\n",
    "# Display Image\n",
    "waxs_analysis.display_image(data, \n",
    "                            title=projectname, \n",
    "                            cmap='turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a .xye File for TOPAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a WAXSTOPAS instance based on the existing WAXSReduce instance\n",
    "waxs_topas = WAXSTOPAS(projectPath=projectPath, waxs_instance=waxs_analysis)\n",
    "\n",
    "# Now you can call the .xye generation method\n",
    "waxs_topas.generate_xye_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a .int pXRD Simulated File for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intPath = pathlib.Path('/Users/keithwhite/github_repositories/pyWAXS/examples_local/projects/AgBh_Calib/poscar/agbh_cif.int')\n",
    "wavelength = 0.9763 # Å^-1\n",
    "\n",
    "# Load the data\n",
    "loaded_data = load_int_file(intPath, wavelength)\n",
    "loaded_data.head(), loaded_data.attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the pXRD .int Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the figure\n",
    "%matplotlib widget\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# Boolean variable to toggle normalization\n",
    "normalize_traces = True\n",
    "\n",
    "qr_min = float(0.1)\n",
    "qr_max = float(3)\n",
    "\n",
    "filtered_data = loaded_data[(loaded_data['q'] >= qr_min) & (loaded_data['q'] <= qr_max)]\n",
    "\n",
    "if normalize_traces:\n",
    "    plt.plot(filtered_data['q'], filtered_data['intensity'] / filtered_data['intensity'].max(), label='Loaded .int File')\n",
    "else:\n",
    "    plt.plot(filtered_data['q'], filtered_data['intensity'], label='Loaded .int File')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('q / qr')\n",
    "plt.ylabel('Intensity')\n",
    "plt.title('1D Integrated Data Comparison')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the Data & pXRD Simulation Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the figure\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# Boolean variable to toggle normalization\n",
    "normalize_traces = True\n",
    "\n",
    "# methods = ['lut', 'bbox']\n",
    "methods = ['bbox']\n",
    "for method in methods:\n",
    "    waxs_analysis.run_pg_integrate1D(npt=2048, method=method, correctSolidAngle=True, polarization_factor=-0)\n",
    "    intensity_values = waxs_analysis.integrate1d_da.values\n",
    "    if normalize_traces:\n",
    "        intensity_values /= intensity_values.max()\n",
    "    plt.plot(waxs_analysis.integrate1d_da.qr, intensity_values, label=f'pygix {method}')\n",
    "\n",
    "qr_min = float(waxs_analysis.integrate1d_da.qr.min())\n",
    "qr_max = float(waxs_analysis.integrate1d_da.qr.max())\n",
    "filtered_data = loaded_data[(loaded_data['q'] >= qr_min) & (loaded_data['q'] <= qr_max)]\n",
    "\n",
    "if normalize_traces:\n",
    "    plt.plot(filtered_data['q'], filtered_data['intensity'] / filtered_data['intensity'].max(), label='Loaded .int File')\n",
    "else:\n",
    "    plt.plot(filtered_data['q'], filtered_data['intensity'], label='Loaded .int File')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('q / qr')\n",
    "plt.ylabel('Intensity')\n",
    "plt.title('1D Integrated Data Comparison')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Analysis Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sin(chi) Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caked Image Sin-Chi Correction: Apply sin(chi) correction to the caked image.\n",
    "cakedtiff_sinchi_xr = waxs_analysis.sinchi_corr(chicorr = True, \n",
    "                                                qsqr = False)\n",
    "\n",
    "# Image Normalization: Normalize the caked image.\n",
    "cakedtiff_xr_norm, (max_x, max_y) = waxs_analysis.normalize_image(img = waxs_analysis.cakedtiff_sinchi_xr, \n",
    "                                                  normalizerecip=False)\n",
    "data = cakedtiff_xr_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Folding: Fold the caked image.\n",
    "folded_data = waxs_analysis.fold_image(data, 'chi')\n",
    "data = folded_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = waxs_analysis.cakedtiff_xr\n",
    "\n",
    "# Image Normalization: Normalize the caked image.\n",
    "data, (max_x, max_y) = waxs_analysis.normalize_image(img = data, \n",
    "                                                  normalizerecip=False)\n",
    "\n",
    "# Image Interpolation: Interpolate the folded image.\n",
    "# interpolator = ImageInterpolator() # Create the interpolator object. \n",
    "# data = interpolator.simple_interpolate(data, 'horizontal', 'slinear') # interpolate horizontal gaps with slinear method.\n",
    "# data = interpolator.simple_interpolate(data, 'vertical', 'slinear') # interpolate unfilled vertical gaps with slinear method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Folding: Fold the caked image.\n",
    "data = waxs_analysis.cakedtiff_xr\n",
    "data = waxs_analysis.fold_image(data, 'chi')\n",
    "data, (max_x, max_y) = waxs_analysis.normalize_image(img = data, \n",
    "                                                  normalizerecip=False)\n",
    "\n",
    "# Display Image\n",
    "waxs_analysis.display_image(data, \n",
    "                            title='folded', \n",
    "                            cmap='turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peak Finding\n",
    "Find the peaks using a our custom peak finding algorithm.\n",
    "\n",
    "Parameters:\n",
    "- sigma1 (float, default=1.0): The standard deviation for the first Gaussian filter.\n",
    "- sigma2 (float, default=2.0): The standard deviation for the second Gaussian filter.\n",
    "- threshold (float, default=0.2): Threshold for initial peak identification.\n",
    "- clustering_method (str, default='DBSCAN'): The clustering method to use ('DBSCAN' or 'HDBSCAN').\n",
    "- eps (float, default=3): The maximum distance between two samples for them to be considered as in the same cluster (DBSCAN).\n",
    "- min_samples (int, default=2): The number of samples in a neighborhood for a point to be considered as a core point (DBSCAN).\n",
    "- k (int, default=3): The number of nearest neighbors to consider for the recentering algorithm.\n",
    "- radius (float, default=5): The radius within which to search for neighbors in the recentering algorithm.\n",
    "- edge_percentage (float, default=5): The percentage of the minimum edge length to be considered as the edge zone.\n",
    "- stricter_threshold (float, default=0.01): A stricter threshold for edge peaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "data = waxs_analysis.reciptiff_xr\n",
    "# data = waxs_analysis.fold_image(data, 'chi')\n",
    "# data, (max_x, max_y) = waxs_analysis.normalize_image(img = data, \n",
    "#                                                   normalizerecip=False)\n",
    "\n",
    "# Find peaks (implement the actual peak-finding logic in the find_peaks method)\n",
    "peak_finder = WAXSSearch(data) # create the WAXSSearch object\n",
    "\n",
    "# NOTE: Make sure you pass the ACTIVE DataArray you are working on, notice how I passed 'data' \n",
    "# since this is the processed dataset we are working with locally.\n",
    "\n",
    "const_params = {\n",
    "    'threshold': 0.0005,\n",
    "    'clustering_method': 'HDBSCAN',\n",
    "    'eps': 1,\n",
    "    'min_samples': 2,\n",
    "    'k': 4,\n",
    "    'radius': 2,\n",
    "    'edge_percentage': 2,\n",
    "    'stricter_threshold': 20\n",
    "}\n",
    "\n",
    "dataset = peak_finder.waxssearch_main(sigma1=.15,\n",
    "                                        sigma2=2.91,\n",
    "                                        **const_params)\n",
    "\n",
    "# Display image with peaks\n",
    "peak_finder.display_image_with_peaks_and_DoG(dataset,\n",
    "                                       title='peak finder', \n",
    "                                       cmap='turbo')\n",
    "\n",
    "# peak_finder.save_to_netcdf(hdf5Path) # Creates an hdf5 file with the name 'output.nc' in your output path. Will generalize\n",
    "# the file naming convention for this soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Peak Search Parameter Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Import the WAXSSearch class and any other dependencies here\n",
    "\n",
    "# Define constant parameters\n",
    "const_params = {\n",
    "    'threshold': 0.0005,\n",
    "    'clustering_method': 'HDBSCAN',\n",
    "    'eps': 1,\n",
    "    'min_samples': 2,\n",
    "    'k': 4,\n",
    "    'radius': 2,\n",
    "    'edge_percentage': 2,\n",
    "    'stricter_threshold': 5\n",
    "}\n",
    "\n",
    "# Assume that 'data' is available here or load it\n",
    "\n",
    "# Define ranges for sigma1 and sigma2\n",
    "sigma1_values = np.linspace(0.1, 1.2, 20)\n",
    "sigma2_values = np.linspace(1.3, 3, 20)\n",
    "\n",
    "# Create a matrix to store the results\n",
    "num_peaks_matrix = np.zeros((len(sigma1_values), len(sigma2_values)))\n",
    "\n",
    "# Run the simulation\n",
    "for i, sigma1 in enumerate(sigma1_values):\n",
    "    for j, sigma2 in enumerate(sigma2_values):\n",
    "        # Create a WAXSSearch object with the current 'data'\n",
    "        peak_finder = WAXSSearch(data)\n",
    "        \n",
    "        # Run the actual waxssearch_main method\n",
    "        dataset = peak_finder.waxssearch_main(sigma1=sigma1,\n",
    "                                              sigma2=sigma2,\n",
    "                                              **const_params)\n",
    "        \n",
    "        # Extract the number of peaks found\n",
    "        num_peaks = np.count_nonzero(dataset['peak_positions'].values == 1)\n",
    "        num_peaks_matrix[i, j] = num_peaks\n",
    "\n",
    "\n",
    "# Convert to DataFrame for better annotation in heatmap\n",
    "df_num_peaks = pd.DataFrame(num_peaks_matrix, index=np.round(sigma1_values, 2), columns=np.round(sigma2_values, 2))\n",
    "\n",
    "plt.close('all')\n",
    "\n",
    "# Generate the heatmap\n",
    "sns.heatmap(df_num_peaks, annot=True, fmt=\".0f\", cmap=\"YlGnBu\")\n",
    "plt.xlabel(\"Sigma2 Values\")\n",
    "plt.ylabel(\"Sigma1 Values\")\n",
    "plt.title(\"Heatmap of Number of Peaks Detected\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio  # for creating GIF\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Import WAXSSearch and other dependencies here\n",
    "# ...\n",
    "\n",
    "# Assume that 'data' is available here or load it\n",
    "# ...\n",
    "\n",
    "# Define constant parameters\n",
    "const_params = {\n",
    "    'clustering_method': 'HDBSCAN',\n",
    "    'eps': 1,\n",
    "    'min_samples': 2,\n",
    "    'k': 4,\n",
    "    'radius': 2,\n",
    "    'edge_percentage': 2,\n",
    "    'stricter_threshold': 5\n",
    "}\n",
    "\n",
    "# Define ranges for sigma1, sigma2, and threshold\n",
    "sigma1_values = np.linspace(0.1, 1.2, 20)\n",
    "sigma2_values = np.linspace(1.3, 3, 20)\n",
    "threshold_values = np.linspace(0.0001, 0.001, 10)  # Replace with your range\n",
    "\n",
    "# Initialize a list to store heatmap frames\n",
    "frames = []\n",
    "\n",
    "# Loop over different threshold values\n",
    "for t, threshold in enumerate(threshold_values):\n",
    "    num_peaks_matrix = np.zeros((len(sigma1_values), len(sigma2_values)))\n",
    "\n",
    "    for i, sigma1 in enumerate(sigma1_values):\n",
    "        for j, sigma2 in enumerate(sigma2_values):\n",
    "            if sigma1 >= sigma2:\n",
    "                continue\n",
    "\n",
    "            peak_finder = WAXSSearch(data)\n",
    "            dataset = peak_finder.waxssearch_main(sigma1=sigma1, sigma2=sigma2, threshold=threshold, **const_params)\n",
    "            num_peaks = np.count_nonzero(dataset['peak_positions'].values == 1)\n",
    "            num_peaks_matrix[i, j] = num_peaks\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df_num_peaks = pd.DataFrame(num_peaks_matrix, index=np.round(sigma1_values, 2), columns=np.round(sigma2_values, 2))\n",
    "    \n",
    "    # Generate and save the heatmap\n",
    "    plt.figure()\n",
    "    sns.heatmap(df_num_peaks, annot=True, fmt=\".0f\", cmap=\"YlGnBu\")\n",
    "    plt.xlabel(\"Sigma2 Values\")\n",
    "    plt.ylabel(\"Sigma1 Values\")\n",
    "    plt.title(f\"Heatmap of Number of Peaks Detected (Threshold={threshold})\")\n",
    "    \n",
    "    # Save as PNG\n",
    "    plt.savefig(f\"heatmap_{t}.png\")\n",
    "    \n",
    "    # Append to frames for GIF\n",
    "    frames.append(imageio.imread(f\"heatmap_{t}.png\"))\n",
    "\n",
    "# Create GIF\n",
    "imageio.mimsave('heatmap.gif', frames, duration=1)  # 1-second duration for each frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "# Display GIF in Jupyter Notebook\n",
    "with open('heatmap.gif','rb') as file:\n",
    "    display(Image(file.read()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Existing Project File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# waxsPath = pathlib.Path('/Users/keithwhite/github_repositories/pyWAXS/examples_local/projects/1MAI_3PbI2_DMF_1M_mar23/analysis/hdf5/3MAI_1PbI2_DMF_1M_mar23_peaks.nc')\n",
    "waxsPath = pathlib.Path('/Users/keithwhite/github_repositories/pyWAXS/examples_local/projects/1MAI_3PbI2_DMF_1M_mar23/analysis/hdf5/output.nc')\n",
    "\n",
    "waxs_ds = waxs_analysis.load_xarray_dataset(waxsPath)\n",
    "\n",
    "waxs_ds.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waxs_ds_peaks = WAXSSearch(waxs_ds) # create the WAXSSearch object\n",
    "\n",
    "# Display image with peaks\n",
    "waxs_ds_peaks.display_image_with_peaks(waxs_analysis.ds,\n",
    "                                       title='manual peaks', \n",
    "                                       cmap='turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D Integration Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrator = Integration1D(waxs_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your integrator object 'integrator' has access to and references your active 'waxs_analysis' session. \n",
    "\n",
    "It also has access to all of the WAXSReduce() methods. You can prove it to yourself like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Cake Slide 1D: Display 2D Caked Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrator.display_image(integrator.cakedtiff_xr, # Display the caked data we will manipulate with the 1D image processing.\n",
    "                            title='processed', \n",
    "                            cmap='turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Cake Slice 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrator = Integration1D(waxs_analysis)\n",
    "\n",
    "integrator.cakeslice1D(integrator.cakedtiff_xr, chislice=[-90, 90], qrslice=[0, 4], cakeslicesum='chi')\n",
    "\n",
    "integrator.display_image1D(integrator.cakeslice1D_xr, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Boxcut 1D: Display 2D Recip Space Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "# integrator = Integration1D(waxs_analysis)\n",
    "integrator.display_image(integrator.reciptiff_xr, # Display the caked data we will manipulate with the 1D image processing.\n",
    "                            title='recip space map', \n",
    "                            cmap='turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Box Cut 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated `process_slices` function to dynamically generate a sample name prefix based on the slices\n",
    "def process_slices(integrator, base_samplenameprefix, pngPath, plot_interpolated=False, interpolate_gaps = False, interp_method = 'linear', order = None):\n",
    "    # Qz Layer Lines\n",
    "    qxyslice_qz = [-2.5, 2.5]\n",
    "    qzslice_qz_list = [\n",
    "        [.42, .57],\n",
    "        [.9, 1.07],\n",
    "        [1.42, 1.58],\n",
    "        [1.83, 2.16],\n",
    "        [2.4, 2.6]\n",
    "    ]\n",
    "    \n",
    "    for qzslice in qzslice_qz_list:\n",
    "        dynamic_samplenameprefix = f\"{base_samplenameprefix}_qxyrange{qxyslice_qz[0]}_{qxyslice_qz[1]}_qzrange{qzslice[0]}_{qzslice[1]}\"\n",
    "        title = generate_title('qz', qxyslice_qz, qzslice)\n",
    "        integrator.boxcut1D(integrator.reciptiff_xr, qxyslice=qxyslice_qz, qzslice=qzslice, boxcutsum='qz', interpolate_gaps=interpolate_gaps, interp_method = interp_method, order = None)\n",
    "        integrator.display_image1D(integrator, color='blue', title=title, save_image=True, samplenameprefix=dynamic_samplenameprefix, savePath=pngPath, plot_interpolated=plot_interpolated)\n",
    "        \n",
    "    # Qxy Layer Lines\n",
    "    qzslice_qxy = [0, 3]\n",
    "    qxyslice_qxy_list = [\n",
    "        [-0.95, -0.7],\n",
    "        [-1.18, -1.08],\n",
    "        [-1.7, -1.45],\n",
    "        [-2.06, -1.9],\n",
    "        [-2.24, -2.1],\n",
    "        [-2.36, -2.26]\n",
    "    ]\n",
    "    \n",
    "    for qxyslice in qxyslice_qxy_list:\n",
    "        dynamic_samplenameprefix = f\"{base_samplenameprefix}_qxyrange{qxyslice[0]}_{qxyslice[1]}_qzrange{qzslice_qxy[0]}_{qzslice_qxy[1]}\"\n",
    "        title = generate_title('qxy', qxyslice, qzslice_qxy)\n",
    "        integrator.boxcut1D(integrator.reciptiff_xr, qxyslice=qxyslice, qzslice=qzslice_qxy, boxcutsum='qxy', interpolate_gaps=interpolate_gaps, interp_method = interp_method)\n",
    "        integrator.display_image1D(integrator, color='red', title=title, save_image=True, samplenameprefix=dynamic_samplenameprefix, savePath=pngPath, plot_interpolated=plot_interpolated)\n",
    "        \n",
    "# The generate_title function for reference\n",
    "def generate_title(boxcutsum, qxyslice=None, qzslice=None):\n",
    "    if boxcutsum == 'qz':\n",
    "        title = f\"Qz Layer Lines (Qxy: {qzslice[0]} to {qzslice[1]})\"\n",
    "    elif boxcutsum == 'qxy':\n",
    "        title = f\"Qxy Layer Lines (Qz: {qxyslice[0]} to {qxyslice[1]})\"\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_samplenameprefix = '3MAI1PbI2_DMF_1M'\n",
    "# interp_method = 'cubic', 'slinear', 'nearest', 'zero', 'linear'\n",
    "interp_method = 'linear'\n",
    "process_slices(integrator, base_samplenameprefix, pngPath, plot_interpolated=False, interpolate_gaps = False, interp_method = 'linear', order = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Pole Figure 1D: Display 2D Caked Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrator.display_image(integrator.cakedtiff_xr, # Display the caked data we will manipulate with the 1D image processing.\n",
    "                            title='processed', \n",
    "                            cmap='turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Pole Figure 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrator.polefig1D(integrator.cakedtiff_xr, pole_chislice=[-90, 90], pole_qrslice=[1.8, 1.92], qrcenter=1.85, chicenter=0, poleleveler='linear')\n",
    "integrator.polefig1D(integrator.cakedtiff_xr, pole_chislice=[-90, 90], pole_qrslice=[.62, .75], qrcenter=.67, chicenter=0, poleleveler='average')\n",
    "# integrator.polefig1D(integrator.cakedtiff_xr, pole_chislice=[-90, 90], pole_qrslice=[.62, .75], qrcenter=.67, chicenter=0, poleleveler=None)\n",
    "# integrator.polefig1D(integrator.cakedtiff_xr, pole_chislice=[-90, 90], pole_qrslice=[.62, .75], qrcenter=.67, chicenter=0, poleleveler='linear')\n",
    "\n",
    "integrator.display_image1D(integrator.polefig1D_xr, color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) Azimuthal Integration: Display the 2D Recip Space Map\n",
    "This is effectively the same as the caked integration, but with our own custom pixel splitting routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrator.display_image(integrator.reciptiff_xr, # Display the caked data we will manipulate with the 1D image processing.\n",
    "                            title='recip space map', \n",
    "                            cmap='turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (i) Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "waxs_analysis.rawdisplay_xr()\n",
    "# waxs_analysis.rawtiff_xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (ii) Reciprocal Space Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "waxs_analysis.recipdisplay_xr()\n",
    "# waxs_analysis.reciptiff_xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (iii) Caked Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "waxs_analysis.cakeddisplay_xr()\n",
    "# waxs_analysis.cakedtiff_xr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyGIXS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
