{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!keithwhite@Keiths-MacBook-Pro/opt/anaconda3/envs/pyWAXS\n",
    "# --------\n",
    "# keith white\n",
    "# date: 05/17/2023\n",
    "# script: waxssim_main.py\n",
    "# --\n",
    "# purpose: simulate giwaxs data from inputted poscar file. import real data to compare\n",
    "# to simulated data. select regions of interest in the real data to compare to the \n",
    "# simulated data. set a number of initial guesses for to the crystallite \n",
    "# orientation parameters. run a chi-square minimization routine to fit the \n",
    "# simulated data to the real data.\n",
    "# --------\n",
    "\n",
    "# -- functions -- #\n",
    "# -- simulated data\n",
    "# load poscar file\n",
    "# read-in poscar file\n",
    "# input parameters\n",
    "    # crystallite orientation parameters\n",
    "    # extent of (h k l) \n",
    "    # image resolution\n",
    "    # scherrer grain-size analysis\n",
    "# generate intensity map\n",
    "\n",
    "# -- real data\n",
    "# read-in .tiff file image\n",
    "# create a detector corrections object for .tiff file image corrections\n",
    "# apply detector corrections to the .tiff file image\n",
    "# select regions of interest on the .tiff file image (2 - 4)\n",
    "# find peaks on the tiff image file\n",
    "\n",
    "# -- comparator functions\n",
    "# create a project folder that includes CIFs, data, PONI, and corrected image .csv files\n",
    "# check found peaks against simulated peak positions - group sets with same delta between\n",
    "# reflectons\n",
    "# check (qxy, qz) positions in poscar file against selected peaks in real data set\n",
    "# modify lattice constants (a, b, c, alpha, beta, gamma) to match simulation to the real data\n",
    "# set initial conditions for least squares minimization\n",
    "# modify all gaussian parameters over specified step size in sigma, creating\n",
    "# a 3D parameter space of textured images. \n",
    "# Use a peak finding algorithm to find the local minima of each mapped parameter space.\n",
    "# Compare all local minima to find the global minima\n",
    "# Output the lattice constants and correction parameters\n",
    "# Output a list of the found peaks in the data, and whether they match the CIF or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import ndimage as ndi\n",
    "from scipy.special import voigt_profile\n",
    "from skimage import io, filters, feature\n",
    "from skimage.feature import peak_local_max\n",
    "\n",
    "def gaussian_noise(image, mean=0, std=1):\n",
    "    return image + np.random.normal(mean, std, image.shape)\n",
    "\n",
    "def generate_image(height, width, num_peaks, max_peak_width, max_peak_height):\n",
    "    image = np.zeros((height, width))\n",
    "    for _ in range(num_peaks):\n",
    "        xc, yc = np.random.randint(0, height), np.random.randint(0, width)\n",
    "        peak_width = np.random.randint(1, max_peak_width)\n",
    "        peak_height = np.random.randint(1, max_peak_height)\n",
    "        y, x = np.mgrid[:height, :width]\n",
    "        image += peak_height * voigt_profile((x-xc)/peak_width, (y-yc)/peak_width, 0)\n",
    "    return image\n",
    "\n",
    "def find_peaks(image, min_distance=20, threshold_abs=None):\n",
    "    smoothed_image = filters.gaussian(image, sigma=1)  # Adjust sigma according to your needs\n",
    "    coordinates = peak_local_max(smoothed_image, min_distance=min_distance, threshold_abs=threshold_abs, exclude_border=True) \n",
    "    valid_peaks = []\n",
    "    for coord in coordinates:\n",
    "        x, y = coord\n",
    "        if np.all(smoothed_image[max(0,x-1):min(image.shape[0],x+2), max(0,y-1):min(image.shape[1],y+2)] > 0):\n",
    "            valid_peaks.append(coord)\n",
    "    return np.array(valid_peaks)\n",
    "\n",
    "def test_find_peaks():\n",
    "    # Generate an image with random peaks and noise\n",
    "    image = generate_image(200, 200, 10, 20, 50)\n",
    "    image = gaussian_noise(image)\n",
    "    peaks = find_peaks(image)\n",
    "    print(\"Found peaks at: \", peaks)\n",
    "\n",
    "test_find_peaks()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage as ndi\n",
    "from scipy.special import voigt_profile\n",
    "from skimage import io, filters, feature\n",
    "from skimage.feature import peak_local_max\n",
    "\n",
    "def gaussian_noise(image, mean=0, std=1):\n",
    "    return image + np.random.normal(mean, std, image.shape)\n",
    "\n",
    "def generate_image(height, width, num_peaks, max_peak_width, max_peak_height):\n",
    "    image = np.zeros((height, width))\n",
    "    for _ in range(num_peaks):\n",
    "        xc, yc = np.random.randint(0, height), np.random.randint(0, width)\n",
    "        peak_width = np.random.randint(1, max_peak_width)\n",
    "        peak_height = np.random.randint(1, max_peak_height)\n",
    "        y, x = np.mgrid[:height, :width]\n",
    "        image += peak_height * voigt_profile((x-xc)/peak_width, (y-yc)/peak_width, 0)\n",
    "    return image\n",
    "\n",
    "def find_peaks(image, min_distance=20, threshold_abs=None):\n",
    "    smoothed_image = filters.gaussian(image, sigma=1)  # Adjust sigma according to your needs\n",
    "    coordinates = peak_local_max(smoothed_image, min_distance=min_distance, threshold_abs=threshold_abs, exclude_border=True) \n",
    "    valid_peaks = []\n",
    "    for coord in coordinates:\n",
    "        x, y = coord\n",
    "        if np.all(smoothed_image[max(0,x-1):min(image.shape[0],x+2), max(0,y-1):min(image.shape[1],y+2)] > 0):\n",
    "            valid_peaks.append(coord)\n",
    "    return np.array(valid_peaks)\n",
    "\n",
    "def visualize_image_and_peaks(image, peaks):\n",
    "    plt.imshow(image)\n",
    "    plt.scatter(peaks[:, 1], peaks[:, 0], color='red')  # Note the order of the coordinates\n",
    "    plt.show()\n",
    "\n",
    "def test_find_peaks():\n",
    "    # Generate an image with random peaks and noise\n",
    "    image = generate_image(200, 200, 10, 20, 50)\n",
    "    image = gaussian_noise(image)\n",
    "    \n",
    "    # Find and visualize the peaks\n",
    "    peaks = find_peaks(image)\n",
    "    print(\"Found peaks at: \", peaks)\n",
    "    visualize_image_and_peaks(image, peaks)\n",
    "\n",
    "test_find_peaks()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage as ndi\n",
    "from scipy.special import voigt_profile\n",
    "from skimage import io, filters, feature\n",
    "from skimage.feature import peak_local_max\n",
    "%matplotlib widget\n",
    "\n",
    "def gaussian_noise(image, mean=0, std=1):\n",
    "    return image + np.random.normal(mean, std, image.shape)\n",
    "\n",
    "def generate_image(height, width, num_peaks, max_peak_width, max_peak_height):\n",
    "    image = np.zeros((height, width))\n",
    "    for _ in range(num_peaks):\n",
    "        xc, yc = np.random.randint(0, height), np.random.randint(0, width)\n",
    "        peak_width = np.random.randint(1, max_peak_width)\n",
    "        peak_height = np.random.randint(1, max_peak_height)\n",
    "        y, x = np.mgrid[:height, :width]\n",
    "        image += peak_height * voigt_profile((x-xc)/peak_width, (y-yc)/peak_width, 0)\n",
    "    return image\n",
    "\n",
    "def find_peaks(image, min_distance=20, threshold_abs=None):\n",
    "    smoothed_image = filters.gaussian(image, sigma=1)  # Adjust sigma according to your needs\n",
    "    coordinates = peak_local_max(smoothed_image, min_distance=min_distance, threshold_abs=threshold_abs, exclude_border=True) \n",
    "    valid_peaks = []\n",
    "    for coord in coordinates:\n",
    "        x, y = coord\n",
    "        if np.all(smoothed_image[max(0,x-1):min(image.shape[0],x+2), max(0,y-1):min(image.shape[1],y+2)] > 0):\n",
    "            valid_peaks.append(coord)\n",
    "    return np.array(valid_peaks)\n",
    "\n",
    "def visualize_image_and_peaks(image, peaks):\n",
    "    plt.imshow(image, cmap='turbo')#, norm=plt.Normalize(vmin=np.min(image), vmax=np.max(image)))\n",
    "    plt.scatter(peaks[:, 1], peaks[:, 0], color='black')  # Note the order of the coordinates\n",
    "    plt.show()\n",
    "\n",
    "def test_find_peaks(height, width, num_peaks, max_peak_width, max_peak_height):\n",
    "    # Generate an image with random peaks and noise\n",
    "    image = generate_image(height, width, num_peaks, max_peak_width, max_peak_height)\n",
    "    image = gaussian_noise(image)\n",
    "    \n",
    "    # Find and visualize the peaks\n",
    "    peaks = find_peaks(image)\n",
    "    print(\"Found peaks at: \", peaks)\n",
    "    visualize_image_and_peaks(image, peaks)\n",
    "\n",
    "test_find_peaks(200, 200, 10, 80, 200)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage as ndi\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def gaussian_noise(image, mean=0, std=1):\n",
    "    return image + np.random.normal(mean, std, image.shape)\n",
    "\n",
    "def generate_image(height, width, num_peaks, max_peak_width, max_peak_height):\n",
    "    image = np.zeros((height, width))\n",
    "    for _ in range(num_peaks):\n",
    "        xc, yc = np.random.randint(0, height), np.random.randint(0, width)\n",
    "        peak_width = np.random.randint(1, max_peak_width)\n",
    "        peak_height = np.random.randint(1, max_peak_height)\n",
    "        x, y = np.meshgrid(np.linspace(0, width, width), np.linspace(0, height, height))\n",
    "        pos = np.empty(x.shape + (2,))\n",
    "        pos[:, :, 0] = x; pos[:, :, 1] = y\n",
    "        rv = multivariate_normal([xc, yc], [[peak_width, 0], [0, peak_width]])\n",
    "        image += peak_height * rv.pdf(pos)\n",
    "    return image\n",
    "\n",
    "def find_peaks(image, size=3):\n",
    "    maxima = (image == ndi.maximum_filter(image, size=size))\n",
    "    labels, num_labels = ndi.label(maxima)\n",
    "    peak_coords = np.array(ndi.center_of_mass(image, labels, range(1, num_labels+1)))\n",
    "    return peak_coords\n",
    "\n",
    "def visualize_image_and_peaks(image, peaks):\n",
    "    plt.imshow(image, cmap='turbo', norm=plt.Normalize(vmin=np.min(image), vmax=np.max(image)))\n",
    "    plt.scatter(peaks[:, 1], peaks[:, 0], color='black')  # Note the order of the coordinates\n",
    "    plt.show()\n",
    "\n",
    "def test_find_peaks(height, width, num_peaks, max_peak_width, max_peak_height):\n",
    "    # Generate an image with random peaks and noise\n",
    "    image = generate_image(height, width, num_peaks, max_peak_width, max_peak_height)\n",
    "    image = gaussian_noise(image)\n",
    "    \n",
    "    # Find and visualize the peaks\n",
    "    peaks = find_peaks(image)\n",
    "    print(\"Found peaks at: \", peaks)\n",
    "    visualize_image_and_peaks(image, peaks)\n",
    "\n",
    "test_find_peaks(200, 200, 10, 20, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage as ndi\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "def gaussian_noise(image, mean=0, std=.1):\n",
    "    return image + np.random.normal(mean, std, image.shape)\n",
    "\n",
    "def random_gaussians(size=1000, num_gaussians=5, max_intensity=1):\n",
    "    # Create a grid of coordinates for the array\n",
    "    x, y = np.meshgrid(np.arange(size), np.arange(size))\n",
    "\n",
    "    # Add multiple random Gaussian distributions to the array\n",
    "    array = np.zeros((size, size))\n",
    "    for _ in range(num_gaussians):\n",
    "        x0, y0 = size * np.random.rand(2)\n",
    "        x_sigma, y_sigma = size * (0.05 + 0.15*np.random.rand(2))\n",
    "        amplitude = max_intensity / num_gaussians\n",
    "        array += amplitude * np.exp(-((x-x0)**2/(2*x_sigma**2) + (y-y0)**2/(2*y_sigma**2)))\n",
    "\n",
    "    return array\n",
    "\n",
    "def plot_gaussians(array, points=None):\n",
    "    plt.close('all')\n",
    "    plt.imshow(array, cmap='turbo')\n",
    "    plt.colorbar()\n",
    "    if points is not None:\n",
    "        plt.plot(points[:, 1], points[:, 0], 'ko', markersize=2)\n",
    "    plt.show()\n",
    "\n",
    "def find_peaks(image, size=3, smoothing_sigma=1, relative_threshold=1.5):\n",
    "    # Smooth the image using a Gaussian filter\n",
    "    smoothed = gaussian_filter(image, sigma=smoothing_sigma)\n",
    "    \n",
    "    # Find local maxima\n",
    "    maxima = (smoothed == ndi.maximum_filter(smoothed, size=size))\n",
    "    labels, num_labels = ndi.label(maxima)\n",
    "    \n",
    "    # Compute the noise level (standard deviation) in the image\n",
    "    noise_level = np.std(image)\n",
    "    \n",
    "    # Determine a suitable absolute threshold based on the noise level\n",
    "    absolute_threshold = relative_threshold * noise_level\n",
    "    \n",
    "    # Find the coordinates of the peaks that are above the threshold\n",
    "    peak_coords = np.array(ndi.center_of_mass(image, labels, range(1, num_labels+1)))\n",
    "    peak_values = smoothed[tuple(peak_coords.astype(int).T)]\n",
    "    peak_coords = peak_coords[peak_values > absolute_threshold]\n",
    "    \n",
    "    return peak_coords\n",
    "\n",
    "# Now the test function\n",
    "def test_find_peaks(size, num_gaussians, max_intensity):\n",
    "    # Generate an image with random peaks and noise\n",
    "    image = random_gaussians(size, num_gaussians, max_intensity)\n",
    "    image = gaussian_noise(image)\n",
    "    \n",
    "    # Find and visualize the peaks\n",
    "    peaks = find_peaks(image)\n",
    "    print(\"Found peaks at: \", peaks)\n",
    "    plot_gaussians(image, peaks)\n",
    "    \n",
    "test_find_peaks(200, 10, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage as ndi\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.cluster import DBSCAN\n",
    "from skimage.restoration import estimate_sigma\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "def gaussian_noise(image, mean=0, std=.1):\n",
    "    return image + np.random.normal(mean, std, image.shape)\n",
    "\n",
    "def random_gaussians(size=1000, num_gaussians=5, max_intensity=1):\n",
    "    # Create a grid of coordinates for the array\n",
    "    x, y = np.meshgrid(np.arange(size), np.arange(size))\n",
    "\n",
    "    # Add multiple random Gaussian distributions to the array\n",
    "    array = np.zeros((size, size))\n",
    "    for _ in range(num_gaussians):\n",
    "        x0, y0 = size * np.random.rand(2)\n",
    "        x_sigma, y_sigma = size * (0.05 + 0.15*np.random.rand(2))\n",
    "        amplitude = max_intensity / num_gaussians\n",
    "        array += amplitude * np.exp(-((x-x0)**2/(2*x_sigma**2) + (y-y0)**2/(2*y_sigma**2)))\n",
    "\n",
    "    return array\n",
    "\n",
    "def plot_gaussians(array, points=None):\n",
    "    plt.close('all')\n",
    "    plt.imshow(array, cmap='turbo')\n",
    "    plt.colorbar()\n",
    "    if points is not None:\n",
    "        plt.plot(points[:, 1], points[:, 0], 'ko', markersize=2)\n",
    "    plt.show()\n",
    "\n",
    "def cluster_peaks(peak_coords, eps=3, min_samples=2):\n",
    "    # Apply DBSCAN to the peak coordinates\n",
    "    clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(peak_coords)\n",
    "    \n",
    "    # Compute the centroid of each cluster\n",
    "    clustered_peaks = []\n",
    "    for cluster_id in np.unique(clustering.labels_):\n",
    "        if cluster_id == -1:  # -1 means noise in DBSCAN\n",
    "            continue\n",
    "        coords_in_cluster = peak_coords[clustering.labels_ == cluster_id]\n",
    "        centroid = np.mean(coords_in_cluster, axis=0)\n",
    "        clustered_peaks.append(centroid)\n",
    "        \n",
    "    return np.array(clustered_peaks)\n",
    "\n",
    "def calculate_snr(image):\n",
    "    # Signal is the maximum pixel value\n",
    "    signal = np.max(image)\n",
    "    \n",
    "    # Noise is estimated as the standard deviation of the pixel values\n",
    "    noise = estimate_sigma(image, multichannel=False)\n",
    "    \n",
    "    # Calculate the SNR\n",
    "    snr = signal / noise\n",
    "    return snr\n",
    "\n",
    "def find_peaks(image, size=3, smoothing_sigma=1, relative_threshold=None):\n",
    "    # Smooth the image using a Gaussian filter\n",
    "    smoothed = gaussian_filter(image, sigma=smoothing_sigma)\n",
    "    \n",
    "    # Find local maxima\n",
    "    maxima = (smoothed == ndi.maximum_filter(smoothed, size=size))\n",
    "    labels, num_labels = ndi.label(maxima)\n",
    "    \n",
    "    # Compute the noise level (standard deviation) in the image\n",
    "    noise_level = estimate_sigma(image, multichannel=False)\n",
    "    \n",
    "    # If relative_threshold is None, set it based on the SNR\n",
    "    if relative_threshold is None:\n",
    "        snr = calculate_snr(image)\n",
    "        relative_threshold = 1.5 if snr < 1 else 1/snr\n",
    "    \n",
    "    # Determine a suitable absolute threshold based on the noise level\n",
    "    absolute_threshold = relative_threshold * noise_level\n",
    "    \n",
    "    # Find the coordinates of the peaks that are above the threshold\n",
    "    peak_coords = np.array(ndi.center_of_mass(image, labels, range(1, num_labels+1)))\n",
    "    peak_values = smoothed[tuple(peak_coords.astype(int).T)]\n",
    "    peak_coords = peak_coords[peak_values > absolute_threshold]\n",
    "    \n",
    "    return peak_coords\n",
    "\n",
    "def test_find_peaks(size, num_gaussians, max_intensity, size_peak=3, smoothing_sigma=1, relative_threshold=None, eps=3, min_samples=2):\n",
    "    # Generate an image with random peaks and noise\n",
    "    image = random_gaussians(size, num_gaussians, max_intensity)\n",
    "    image = gaussian_noise(image)\n",
    "    \n",
    "    # Find and cluster the peaks\n",
    "    peaks = find_peaks(image, size_peak, smoothing_sigma, relative_threshold)\n",
    "    clustered_peaks = cluster_peaks(peaks, eps, min_samples)\n",
    "    \n",
    "    print(\"Found peaks at: \", clustered_peaks)\n",
    "    plot_gaussians(image, clustered_peaks)\n",
    "\n",
    "test_find_peaks(200, 10, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage as ndi\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.cluster import DBSCAN\n",
    "from skimage.restoration import estimate_sigma\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "%matplotlib widget\n",
    "\n",
    "def gaussian_noise(image, mean=0, std=.1):\n",
    "    return image + np.random.normal(mean, std, image.shape)\n",
    "\n",
    "def random_gaussians(size=1000, num_gaussians=5, max_intensity=1):\n",
    "    # Create a grid of coordinates for the array\n",
    "    x, y = np.meshgrid(np.arange(size), np.arange(size))\n",
    "\n",
    "    # Add multiple random Gaussian distributions to the array\n",
    "    array = np.zeros((size, size))\n",
    "    for _ in range(num_gaussians):\n",
    "        x0, y0 = size * np.random.rand(2)\n",
    "        x_sigma, y_sigma = size * (0.05 + 0.15*np.random.rand(2))\n",
    "        amplitude = max_intensity / num_gaussians\n",
    "        array += amplitude * np.exp(-((x-x0)**2/(2*x_sigma**2) + (y-y0)**2/(2*y_sigma**2)))\n",
    "\n",
    "    return array\n",
    "\n",
    "def plot_gaussians(array, points=None):\n",
    "    plt.close('all')\n",
    "    plt.imshow(array, cmap='turbo')\n",
    "    plt.colorbar()\n",
    "    if points is not None:\n",
    "        plt.plot(points[:, 1], points[:, 0], 'ko', markersize=2)\n",
    "    plt.show()\n",
    "\n",
    "def cluster_peaks(peak_coords, eps=3, min_samples=2):\n",
    "    # Apply DBSCAN to the peak coordinates\n",
    "    clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(peak_coords)\n",
    "    \n",
    "    # Compute the centroid of each cluster\n",
    "    clustered_peaks = []\n",
    "    for cluster_id in np.unique(clustering.labels_):\n",
    "        if cluster_id == -1:  # -1 means noise in DBSCAN\n",
    "            continue\n",
    "        coords_in_cluster = peak_coords[clustering.labels_ == cluster_id]\n",
    "        centroid = np.mean(coords_in_cluster, axis=0)\n",
    "        clustered_peaks.append(centroid)\n",
    "        \n",
    "    return np.array(clustered_peaks)\n",
    "\n",
    "def calculate_snr(image):\n",
    "    # Signal is the maximum pixel value\n",
    "    signal = np.max(image)\n",
    "    \n",
    "    # Noise is estimated as the standard deviation of the pixel values\n",
    "    noise = estimate_sigma(image, multichannel=False)\n",
    "    \n",
    "    # Calculate the SNR\n",
    "    snr = signal / noise\n",
    "    return snr\n",
    "\n",
    "def find_peaks(image, size=3, smoothing_sigma=1, relative_threshold=None):\n",
    "    # Smooth the image using a Gaussian filter\n",
    "    smoothed = gaussian_filter(image, sigma=smoothing_sigma)\n",
    "    \n",
    "    # Find local maxima\n",
    "    maxima = (smoothed == ndi.maximum_filter(smoothed, size=size))\n",
    "    labels, num_labels = ndi.label(maxima)\n",
    "    \n",
    "    # Compute the noise level (standard deviation) in the image\n",
    "    noise_level = estimate_sigma(image, multichannel=False)\n",
    "    \n",
    "    # If relative_threshold is None, set it based on the SNR\n",
    "    if relative_threshold is None:\n",
    "        snr = calculate_snr(image)\n",
    "        relative_threshold = 1.5 if snr < 1 else 1/snr\n",
    "    \n",
    "    # Determine a suitable absolute threshold based on the noise level\n",
    "    absolute_threshold = relative_threshold * noise_level\n",
    "    \n",
    "    # Find the coordinates of the peaks that are above the threshold\n",
    "    peak_coords = np.array(ndi.center_of_mass(image, labels, range(1, num_labels+1)))\n",
    "    peak_values = smoothed[tuple(peak_coords.astype(int).T)]\n",
    "    peak_coords = peak_coords[peak_values > absolute_threshold]\n",
    "    \n",
    "    return peak_coords\n",
    "\n",
    "def test_find_peaks(size, num_gaussians, max_intensity, size_peak=3, smoothing_sigma=1, relative_threshold=None, eps=3, min_samples=2):\n",
    "    # Generate an image with random peaks and noise\n",
    "    image = random_gaussians(size, num_gaussians, max_intensity)\n",
    "    image = gaussian_noise(image)\n",
    "    \n",
    "    # Find and cluster the peaks\n",
    "    peaks = find_peaks(image, size_peak, smoothing_sigma, relative_threshold)\n",
    "    clustered_peaks = cluster_peaks(peaks, eps, min_samples)\n",
    "    \n",
    "    print(\"Found peaks at: \", clustered_peaks)\n",
    "    plot_gaussians(image, clustered_peaks)\n",
    "\n",
    "# Define a custom Dataset class for your images and peaks\n",
    "class ImagePeakDataset(Dataset):\n",
    "    def __init__(self, images, peaks):\n",
    "        self.images = images\n",
    "        self.peaks = peaks\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1]\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.transform(self.images[idx])\n",
    "        peak = torch.tensor(self.peaks[idx], dtype=torch.float32)\n",
    "        return image, peak\n",
    "\n",
    "# Define a simple CNN architecture\n",
    "class PeakFinderCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PeakFinderCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Create your dataset and data loader\n",
    "dataset = ImagePeakDataset(images, peaks)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize your model, criterion and optimizer\n",
    "model = PeakFinderCNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, peaks) in enumerate(data_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, peaks)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyWAXS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
